{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# SpaceNet7 Change Detection\nProject task: train neural network for semantic segmentation of urban development change","metadata":{}},{"cell_type":"code","source":"! pip install --upgrade rasterio\n! pip install -U git+https://github.com/qubvel/segmentation_models.pytorch\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:32:55.280619Z","iopub.execute_input":"2021-06-02T14:32:55.280927Z","iopub.status.idle":"2021-06-02T14:33:24.921841Z","shell.execute_reply.started":"2021-06-02T14:32:55.280851Z","shell.execute_reply":"2021-06-02T14:33:24.920998Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: rasterio in /opt/conda/lib/python3.7/site-packages (1.2.0)\nCollecting rasterio\n  Downloading rasterio-1.2.4-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n\u001b[K     |████████████████████████████████| 19.3 MB 8.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rasterio) (1.19.5)\nRequirement already satisfied: click>=4.0 in /opt/conda/lib/python3.7/site-packages (from rasterio) (7.1.2)\nRequirement already satisfied: affine in /opt/conda/lib/python3.7/site-packages (from rasterio) (2.3.0)\nRequirement already satisfied: click-plugins in /opt/conda/lib/python3.7/site-packages (from rasterio) (1.1.1)\nRequirement already satisfied: snuggs>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from rasterio) (1.4.7)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from rasterio) (49.6.0.post20201009)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from rasterio) (2020.12.5)\nRequirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from rasterio) (20.3.0)\nRequirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from rasterio) (0.7.1)\nRequirement already satisfied: pyparsing>=2.1.6 in /opt/conda/lib/python3.7/site-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\nInstalling collected packages: rasterio\n  Attempting uninstall: rasterio\n    Found existing installation: rasterio 1.2.0\n    Uninstalling rasterio-1.2.0:\n      Successfully uninstalled rasterio-1.2.0\nSuccessfully installed rasterio-1.2.4\n\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\nCollecting git+https://github.com/qubvel/segmentation_models.pytorch\n  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-88ijmg_i\n  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-88ijmg_i\nRequirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.1.3) (0.8.1)\nCollecting pretrainedmodels==0.7.4\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 311 kB/s eta 0:00:01\n\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\nCollecting timm==0.3.2\n  Downloading timm-0.3.2-py3-none-any.whl (244 kB)\n\u001b[K     |████████████████████████████████| 244 kB 584 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.1.3) (1.7.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (4.55.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.1.3) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.1.3) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.1.3) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.1.3) (1.19.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation-models-pytorch==0.1.3) (7.2.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.1.3) (1.15.0)\nBuilding wheels for collected packages: segmentation-models-pytorch, efficientnet-pytorch, pretrainedmodels\n  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.1.3-py3-none-any.whl size=83164 sha256=d3bd7522593b34d79c64c3b68d7bfc454e83bddb8b5c58e6aa7a6435830110a1\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_qhidr1k/wheels/fa/c5/a8/1e8af6cb04a0974db8a4a156ebd2fdd1d99ad2558d3fce49d4\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=4f2a4354d1a2e2b9c45cb0043c6e71edaaf0fe393be38e9fbd032ca11d673d7d\n  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60963 sha256=850e4118abd19976891b2ddfc8520d45636958b40d18733b68b20ca81df2d069\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\nSuccessfully built segmentation-models-pytorch efficientnet-pytorch pretrainedmodels\nInstalling collected packages: timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\nSuccessfully installed efficientnet-pytorch-0.6.3 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.3 timm-0.3.2\n\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # image plotting\nimport matplotlib\nmatplotlib.rcParams['figure.dpi'] = 300 #increase plot resolution\n\n#Raster data handling\nfrom PIL import Image\nimport skimage\nfrom skimage import io, transform \nimport rasterio as rio # geo-oriented plotting library\nfrom rasterio import features\nimport cv2\n\n#PyTorch\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, Sampler # custom dataset handling\nimport torch.autograd.profiler as profiler # to track model inference and detect leaks\nfrom torchvision import transforms, utils\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.nn.modules.padding import ReplicationPad2d\nimport torchvision.models as models\nfrom torch import optim\nfrom collections import OrderedDict\nimport segmentation_models_pytorch as smp #semantic segmentation models and utils\nfrom torch.cuda.amp import GradScaler\nfrom torch.cuda.amp import autocast\n\n#Augmentations\nimport albumentations as alb\nfrom albumentations.pytorch import ToTensorV2\n\n#Logging errors and progress, sending them to tg-bot\nimport requests\nimport traceback\n\n#Other\nfrom pathlib import Path # to have path strings as PosixPath objexts\nimport pathlib \nfrom pyproj import CRS\nimport geopandas as gpd # to make dataframes out of geojson files\nimport itertools\nimport re\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\nimport os\nimport gc\nfrom timeit import default_timer as time\nimport copy\nimport json\nimport logging\nimport datetime\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T14:33:24.925582Z","iopub.execute_input":"2021-06-02T14:33:24.925857Z","iopub.status.idle":"2021-06-02T14:33:30.215604Z","shell.execute_reply.started":"2021-06-02T14:33:24.925829Z","shell.execute_reply":"2021-06-02T14:33:30.214600Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tqdm/std.py:701: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:33:30.217071Z","iopub.execute_input":"2021-06-02T14:33:30.217632Z","iopub.status.idle":"2021-06-02T14:33:30.221947Z","shell.execute_reply.started":"2021-06-02T14:33:30.217591Z","shell.execute_reply":"2021-06-02T14:33:30.220895Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"# Input\ntrain_dir = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train')\ntest_dir = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_test_public')\nsample_dir = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample')\n\n# Output\noutput_path = Path.cwd()\noutput_csv_path = output_path/'output_csvs/'\nPath(output_csv_path).mkdir(parents=True, exist_ok=True)\n\n# Тестовые данные: растры, geojson и geodataframe с интервалом в 24 месяца\ntest_raster_path = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample/sample/L15-0506E-1204N_2027_3374_13/images_masked/global_monthly_2018_01_mosaic_L15-0506E-1204N_2027_3374_13.tif')\ntest_raster_path_24 = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample/sample/L15-0506E-1204N_2027_3374_13/images_masked/global_monthly_2019_12_mosaic_L15-0506E-1204N_2027_3374_13.tif')\ntest_geojson_path = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample/sample/L15-0506E-1204N_2027_3374_13/labels_match_pix/global_monthly_2018_01_mosaic_L15-0506E-1204N_2027_3374_13_Buildings.geojson')\ntest_geojson_path_24 = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample/sample/L15-0506E-1204N_2027_3374_13/labels_match_pix/global_monthly_2019_12_mosaic_L15-0506E-1204N_2027_3374_13_Buildings.geojson')\ntest_gdf = gpd.read_file(test_geojson_path)\ntest_gdf_24 = gpd.read_file(test_geojson_path_24)\n\n# Делаем из Id индексы датафрейма и сортируем датафрейм по индексу\ntest_gdf.set_index('Id',inplace=True)\ntest_gdf_24.set_index('Id',inplace=True)\n\ntest_gdf.sort_index(inplace=True)\ntest_gdf_24.sort_index(inplace=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T09:12:41.911972Z","iopub.execute_input":"2021-06-02T09:12:41.912369Z","iopub.status.idle":"2021-06-02T09:12:42.438352Z","shell.execute_reply.started":"2021-06-02T09:12:41.912321Z","shell.execute_reply":"2021-06-02T09:12:42.437412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Metadata Extraction\n\n## Paths metadata\nFirst of all lets collect all existing paths and parse their elements to extract any metadata and significant features.\n\nTypical path looks like this:\n../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train/train/L15-1200E-0847N_4802_4803_13/UDM_masks/global_monthly_2018_05_mosaic_L15-1200E-0847N_4802_4803_13_UDM.tif\n\nand can be divided into following parts:\n* parent directory: ../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train/train/, which can be reduced as just train, test or sample\n* location directory with unique id: L15-1200E-0847N_4802_4803_13 which always has 28 digits\n* directory with name that indicates file content: UDM_masks, images, images_masked, labels, labels_match, labels_match_pix\n* filename which includes year, month, location id and file content \n* file extension (.tif, .csv and so on)","metadata":{}},{"cell_type":"code","source":"def unparse_path(string):\n    pattern = r'/(train|test_public|sample)/(L.+)/(\\w+)/(.+_(\\d+)_(\\d+)_m.+_\\d+_\\d+_\\d+)(?:_(\\w+))?.(\\w+)'\n    match = re.findall(pattern=pattern,string=string)\n    return match[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_string = '../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train/train/L15-0331E-1257N_1327_3160_13/labels_match_pix/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson'\nunparse_path(sample_string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Соответственно при заполнении словаря и переносе его в датафрейм будем использовать следующие ключи:\nparent, location_dir, sub_dir, filename, year, month, file_type, extension","metadata":{}},{"cell_type":"code","source":"def get_paths(path):\n    return [str(pth) for pth in Path.glob(path,pattern = '**/*.*')]\n\npathlist_train = get_paths(train_dir)\npathlist_test = get_paths(test_dir)\npathlist_sample = get_paths(sample_dir)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'train_files: {len(pathlist_train)} \\n test_files: {len(pathlist_test)} \\n sample_files: {len(pathlist_sample)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_meta_from_pathlist(pathlist):\n    keys = ['parent', 'location_dir', 'sub_dir', 'filename', 'year', 'month', 'file_type', 'extension', 'source']\n    temp_dir = {key: [] for key in keys}\n    for path in pathlist:\n        temp_dir['source'].append(path)\n        meta = unparse_path(path)\n        for i,data in enumerate(meta):\n            temp_dir[keys[i]].append(data)\n    return pd.DataFrame(temp_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_meta_df = get_meta_from_pathlist(pathlist_train)\ntest_meta_df = get_meta_from_pathlist(pathlist_test)\nsample_meta_df = get_meta_from_pathlist(pathlist_sample)\nconcatenated_df = pd.concat([train_meta_df, test_meta_df, sample_meta_df]).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_meta_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_meta_df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_meta_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concatenated_df.file_type.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concatenated_df[concatenated_df.file_type == ''].extension.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are some missing values in file_type. With the elimination method and common sense, we understand that these are rasters.","metadata":{}},{"cell_type":"code","source":"train_meta_df.loc[train_meta_df['file_type'] == '', 'file_type'] = 'sat_picture'\ntest_meta_df.loc[test_meta_df['file_type'] == '', 'file_type'] = 'sat_picture'\nsample_meta_df.loc[sample_meta_df['file_type'] == '', 'file_type'] = 'sat_picture'\nconcatenated_df.loc[concatenated_df['file_type'] == '', 'file_type'] = 'sat_picture'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concatenated_df.file_type.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we need to figure out connections between the rasters and masks, udm absence or presence.","metadata":{}},{"cell_type":"code","source":"def get_metadata(path):\n    pathlist = get_paths(path)\n    meta_df = get_meta_from_pathlist(pathlist)\n    meta_df.loc[meta_df['file_type'] == '', 'file_type'] = 'Sat_picture'\n    \n    # Choosing udm masks only and taking the indices\n    condition = (meta_df['sub_dir'] == 'UDM_masks')\n    udm_indices = meta_df.loc[condition].index\n    \n    # Get list of unique file names that have UDMs and taking their list\n    udm_fnames = list(meta_df.loc[udm_indices,'filename'])\n    udm_mask = meta_df['filename'].progress_map(lambda x: x in udm_fnames)\n    \n    # Initialize has_udm feature as False\n    meta_df['has_udm'] = False\n    # Changing has_udm feature for files mentioned in list above\n    meta_df.loc[udm_mask,'has_udm'] = True\n\n    return meta_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Generating dataframes with paths metadata","metadata":{}},{"cell_type":"code","source":"train_meta_df = get_metadata(train_dir)\ntest_meta_df = get_metadata(test_dir)\nsample_meta_df = get_metadata(sample_dir)\nconcatenated_meta_df = pd.concat([train_meta_df, test_meta_df, sample_meta_df]).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concatenated_meta_df.has_udm.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_meta_df.to_csv(output_csv_path/'meta_train.csv')\ntest_meta_df.to_csv(output_csv_path/'meta_test.csv')\nsample_meta_df.to_csv(output_csv_path/'meta_sample.csv')\nconcatenated_meta_df.to_csv(output_csv_path/'meta_concat.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we need to combine untidy dataframe with all existing filepaths to the rasters and masks for different areas of interest. It will help us to form custom torch dataset for model training and validation. Lets recollect how the data is organized:\n\n* there is only images_masked dir inside the test_public\n* train and sample consist of: images, images_masked, labels, labels_match, labels_match_pix dirs\n* next part of the path is the same for all dataset parts\n* despite the paths as adresses we need to mention separately: parent directory as dataset part(train, test, sample), AOI ids, date of the shot, filename \n* there are two kinds of labels: with _Buildings and _UDM postfixes. _Buildings postfix is clear. UDM - unidentified mask and most probably contain glitches related to clouds, aerosols and so on","metadata":{}},{"cell_type":"code","source":"concatenated_meta_df[concatenated_meta_df.has_udm == True].filename.iloc[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_untidy(ser):\n    \"\"\"Function for making untidy data out of tidy dataframe\"\"\"\n    part = ser['parent']\n    aoi = ser['location_dir']\n    year = ser['year']\n    month = ser['month']\n    filename = ser['filename']\n    has_udm = ser['has_udm']\n    \n    #making images_masked adresses\n    images_masked =  aoi + '/images_masked/' + filename + '.tif'\n    \n    # test public has images_masked only\n    if part == 'test_public':\n        images = None\n        labels = None\n        labels_match = None\n        labels_match_pix = None\n        UDM_masks = None\n    else:\n        images = aoi + '/images/' + filename + '.tif'\n        labels = aoi + '/labels/' + filename + '.geojson'\n        labels_match = None\n        labels_match_pix = None\n        \n        if has_udm == True:\n            UDM_masks = aoi + '/UDM_masks/' + filename + '_UDM.tif'\n        else: UDM_masks = None\n        \n        images = aoi + '/images/' + filename + '.tif'\n        labels_buildings = aoi + '/labels/' + filename + '_Buildings.geojson'\n        labels_udm = aoi + '/labels/' + filename + '_UDM.geojson'\n        labels_match = aoi + '/labels_match/' + filename + '_Buildings.geojson'\n        labels_match_pix = aoi + '/labels_match_pix/' + filename + '_Buildings.geojson'\n        \n    keys = ['part', 'aoi', 'filename', 'year', 'month', 'has_udm', 'UDM_masks', 'images', 'images_masked', 'labels', 'labels_match', 'labels_match_pix'] \n    values = [part, aoi, filename, year, month, has_udm, UDM_masks, images, images_masked, labels, labels_match, labels_match_pix]\n    temp_dict = {k:v for k, v in zip(keys,values)}\n    return pd.Series(temp_dict)\n\ndef make_untidy_dataframe(df):\n    return df.progress_apply(lambda x: make_untidy(x), axis = 1).drop_duplicates()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_untidy_df = make_untidy_dataframe(train_meta_df)\ntest_untidy_df = make_untidy_dataframe(test_meta_df)\nsample_untidy_df = make_untidy_dataframe(sample_meta_df)\nconcatenated_untidy_df = pd.concat([train_untidy_df, test_untidy_df, sample_untidy_df]).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_untidy_df.to_csv(output_csv_path/'untidy_train.csv')\ntest_untidy_df.to_csv(output_csv_path/'untidy_test.csv')\nsample_untidy_df.to_csv(output_csv_path/'untidy_sample.csv')\nconcatenated_untidy_df.to_csv(output_csv_path/'untidy_concat.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting meta-data from file names\nAs we mentioned above, filenames have similar structure","metadata":{}},{"cell_type":"code","source":"label_csv_path = Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_csvs/csvs/sn7_train_ground_truth_pix.csv')\ndf = pd.read_csv(label_csv_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.filename.unique())\nprint(len('L15-0506E-1204N_2027_3374_13'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"global_monthly_YYYY_MM_mosaic_AOI_ID.tif\n\nгде:\n* YYYY - 4 digits number of the year\n* MM - 2 digits number of the month\n* AOI_ID - area of interest, location id (28 digits), upper register letters.\n\nLets make helpers to be able to extract these features from filenames as well.","metadata":{}},{"cell_type":"code","source":"def extract_fname_feats(string):\n    \"\"\"Function that extract metadata from filenames by regular expressions\"\"\"\n    date_pattern = r'\\d+'\n    id_pattern = r'_(L.+)'\n    extract = lambda x, y: re.findall(pattern=x,string=y)\n    date = tuple(extract(date_pattern, string)[0:2])\n    aoi = extract(id_pattern, string)[0]\n    \n    feat_dict = {'date':date,\n                'aoi': aoi.replace('.tif','')}\n    return feat_dict\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_str = 'global_monthly_2019_12_mosaic_L15-0506E-1204N_2027_3374_13.tif'\nfeats = extract_fname_feats(test_str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['date'], df['aoi'] = zip(*df['filename'].progress_apply(lambda x: list(extract_fname_feats(x).values())))\ndf['year'], df['month'] = zip(*df['date'].progress_map(lambda x: x[0:2]))\ndf['date'] = df['date'].progress_apply(lambda x: datetime.date(int(x[0]), int(x[1]), 1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can to get a closer look to the dataset.","metadata":{}},{"cell_type":"code","source":"df.aoi.value_counts()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.year.value_counts()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Date range: {df.date.min()} - {df.date.max()}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Raster metadata","metadata":{}},{"cell_type":"code","source":"test_gdf.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Geodataframe includes polygons represented in WKT format (well-known-text), these are primitives to draw bulding masks. df columns are district, raster path, polygon geometry.\n\nLets inspect the rasters' size, number of channels, georeferncing, projection, affine transformations","metadata":{}},{"cell_type":"code","source":"raster = rio.open(test_raster_path)\nprint(raster.meta)\nprint(raster.crs.wkt)\nr = raster.read()\nprint(r.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Raster size: 1024 x 1024 px, 3-4 channels: RGB/alpha-RGB\n* Projection: WGS 84 / Pseudo-Mercator (EPSG:3857)\n* Affine: affine transformation matrix, 6 numbers: a,b,c,d,e,f which make this matrix\n\n| x' |   | a b c | | x |  \n| y' | = | d e f | | y |  \n| 1  |   | 0 0 1 | | 1 |  \n\n x,y - pixel coordinates, x',y' - world coordinates","metadata":{}},{"cell_type":"markdown","source":"# Helpers\n\nRoughly speaking, we need to be able to:\n* plot poligons (separatly and above rasters)\n* make rasters out of vector polygons\n* subtract masks to get change masks\n* make smaller chips out of original pictures","metadata":{}},{"cell_type":"code","source":"def plot_polygons(gdf, fill = False, l_wd = 0.2, axs = None, color = 'yellow'):\n    \"\"\"Function to plot polions of structures\n    gdf: geodataframe with polygons geometry\n    fill: boolean, defines either polygons filled or not\n    l_wd: float, linewidth of the polygon contour\n    axs: ax, variable to specify already existing ax to plot, for example: to overlay a sattelite picture\"\"\"\n    \n    if axs == None:\n        _, ax = plt.subplots(1, figsize = (3,3))\n    \n    for geom in gdf.geometry:\n        if fill:\n            # for highliting above the raster\n            ax = axs\n            patch = PolygonPatch(geom,color=color, linewidth = l_wd)\n            ax.add_patch(patch)\n        else:\n            ax.plot(*geom.exterior.xy,linewidth=l_wd)\n    return(ax)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_polygons(test_gdf)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_satellite(path, gdf = None, fill = False, l_wd = 0.2):\n    \"\"\"Function to plot sattelite image with ability to overlay gdf polygons\n    path: string, path to raster\n    gdf: geodataframe that we are going to use to plot polygons\n    fill: boolean, either polygons are filled or not\n    l_wd: float, linewidth\"\"\"\n    \n    fig, ax = plt.subplots(1, figsize = (3,3))\n    fig.tight_layout() #to adjust subplots layout\n    \n    sat = rio.open(path)\n    sat = sat.read()\n    sat = sat.transpose((1,2,0,))\n    ax.imshow(sat)\n    \n    if gdf is not None:\n        plot_polygons(gdf,fill=fill,axs=ax,l_wd=l_wd)\n        \n    return(ax)\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_satellite(test_raster_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_satellite(test_raster_path, test_gdf, fill = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_mask(source, raster):\n    return features.rasterize(((polygon, 255) for polygon in source['geometry']),out_shape=raster.shape)\n    \n\ndef rasterize_mask(source, raster_path, ftype = 'gdf'):\n    \n    \"\"\"Function that allows us to make rasterized mask\n    ftype: string, 'gdf' or 'geojson', defines what is used for source of geometry: dataframe or json\n    source: string or pandas dataframe object, path to geojson or dataframe vatiable\n    raster_path: path to raster which is used as reference to mask shape\n    \"\"\"\n    \n    if ftype == 'gdf':\n        with rio.open(raster_path) as raster:\n            r = raster.read(1)\n            mask = make_mask(source, r)\n        return mask\n            \n    elif ftype == 'geojson':\n        gdf = gpd.read_file(source)\n        with rio.open(raster_path) as raster:\n            r = raster.read(1)\n            mask = make_mask(gdf, r)\n        return mask\n    \n    else:\n        raise ValueError('ftype is incorrect, it can be either \"gdf\" or \"json\"')\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_mask = rasterize_mask(test_geojson_path, test_raster_path, ftype = 'geojson')\ntest_mask_24 = rasterize_mask(test_geojson_path_24, test_raster_path_24, ftype = 'geojson')\nmasks = [test_mask, test_mask_24]\nmonths = ['month_1', 'month_24']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Mask data type {type(test_mask)}, unique values in mask - {np.unique(test_mask)}, mask shape - {test_mask.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 - background, 255 - mask(building) needs to be normalised (divided by 255)\nLets see the very first and the very last sample masks (2 years interval)","metadata":{}},{"cell_type":"code","source":"_, axs = plt.subplots(1,2, figsize = (10,10))\n_.tight_layout()\nfor i, ax in enumerate(axs):\n    ax.set_title(months[i])\n    ax.imshow(masks[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see some difference with naked eye already. Lets make helper and try to see if there is any need to track which of the buildings are demolished through time.","metadata":{}},{"cell_type":"code","source":"def gdf_difference(gdf1, gdf2):\n    \"\"\"Function that leaves only structures which make difference\n    between two versions of the geodataframe \n    \n    Args:\n    gdf1: first geodataframe with polygons (earlier one)\n    gdf2: second geodataframe with polygons (later one)\n    Return:\n    gdf with polygons that make difference\n    \"\"\"\n    try:\n        gdf1.reset_index(inplace=True,drop=True)\n    except:\n        pass\n    try:\n        gdf2.reset_index(inplace=True,drop=True)\n    except:\n        pass\n    \n    \n    len_1 = len(gdf1)\n    len_2 = len(gdf2)\n    \n    len_diff = abs(len_2-len_1)\n    \n    if len_2 > len_1:\n        start_index = len_2-len_diff\n        diff_gdf = gdf2[start_index:].copy()\n    else:\n        start_index = len_1-len_diff\n        diff_gdf = gdf1[start_index:].copy()\n\n    diff_gdf.reset_index(inplace=True,drop=True)\n        \n    return diff_gdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_difference(gdf1, gdf2):\n    \n    \"\"\"Function that leaves only new structures between two versions of the geodataframe \n    and classifies either structure is built or demolished\n    \n    Args:\n    gdf1: first geodataframe with polygons (earlier one)\n    gdf2: second geodataframe with polygons (later one)\n    Return:\n    gdf with polygons that make difference\n    \"\"\"\n    \n    df1 = gdf1.copy()\n    df2 = gdf2.copy()\n    df1.drop(['area','image_fname','iou_score'], inplace = True, axis = 1, errors = 'ignore')\n    df2.drop(['area','image_fname','iou_score'], inplace = True, axis = 1, errors = 'ignore')\n    df1['time'] = 'before'\n    df2['time'] = 'after'\n    \n    difference = df1.merge(df2, how = 'outer', on = 'geometry')\n    difference = difference[(difference.time_x.isna() == True) | (difference.time_y.isna() == True)]\n    difference['mark'] = difference['time_x'].apply(lambda x: 'before' if x == 'before' else 'after')\n    difference.drop(['time_x', 'time_y'], axis = 1, inplace = True)\n    return difference","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diff = get_difference(test_gdf, test_gdf_24)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we now which buildings are appeared and which are demolished. There are much fewer demolished buildings, and for the sake of simplicity I will neglect them.","metadata":{}},{"cell_type":"code","source":"builded = diff[diff['mark'] == 'after']\ndestroyed = diff[diff['mark'] == 'before']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bld_diff = rasterize_mask(builded,test_raster_path)\ndst_diff = rasterize_mask(destroyed,test_raster_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_,axs = plt.subplots(1,4,figsize=(10,10))\n_.tight_layout()\n\nmasks = [test_mask,test_mask_24, bld_diff, dst_diff]\ntitles = ['month 1', 'month 24', 'builded', 'destroyed']\n\nfor i,ax in enumerate(axs):\n    ax.set_title(titles[i])\n    ax.imshow(masks[i]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we need to make chipmaker to inflate dataset and be able to train deeper model.","metadata":{}},{"cell_type":"code","source":"class ChipCreator():\n    \"\"\"Class that allows us to split bigger satellite picture and mask into smaller pieces\"\"\"\n    \n    def __init__(self, dimension, is_raster = False):\n        self.dimension = dimension\n        self.raster = is_raster\n        \n    def __read_image(self,image):\n        # checks whether image is a path or array\n        if isinstance(image,(pathlib.PurePath,str)):\n                with Image.open(image) as img:\n                    # converts image into np array\n                    np_array = np.array(img)\n                return np_array\n            \n        elif isinstance(image,np.ndarray):\n            return image\n        else:\n            raise ValueError(f\"Expected Path or Numpy array received: {type(image)}\")\n        \n    def make_chips(self, image):\n        \n        #getting image and converting to np.array if necessary\n        np_array = self.__read_image(image)\n        \n        # then get numbers of chips per row and column\n        n_rows = (np_array.shape[0] - 1) // self.dimension + 1\n        n_cols = (np_array.shape[1] - 1) // self.dimension + 1\n        \n        chip_list = [] #\n        for r in range(n_rows):\n            for c in range(n_cols):\n                #starting row and column\n                start_r_idx = r*self.dimension\n                end_r_idx = start_r_idx + self.dimension\n                #ending row and column\n                start_c_idx = c*self.dimension\n                end_c_idx = start_c_idx + self.dimension\n                #cutting fragment by indexes\n                chip = np_array[start_r_idx:end_r_idx,start_c_idx:end_c_idx]\n                \n                if self.raster:\n                    # if raster is True then format is (channels, rows, columns)\n                    # else (rows, columns, channels)\n                    chip = np.moveaxis(chip,-1,0)\n\n                chip_list.append(chip)\n\n        return np.array(chip_list)\n    def __call__(self, image):\n        # slightly different verison of make_chips\n        np_array = self.__read_image(image)\n        n_rows = (np_array.shape[1] - 1) // self.dimension + 1\n        n_cols = (np_array.shape[2] - 1) // self.dimension + 1\n        chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n        for r in range(n_rows):\n            for c in range(n_cols):\n                start_r_idx = r*self.dimension\n                end_r_idx = start_r_idx + self.dimension\n\n                start_c_idx = c*self.dimension\n                end_c_idx = start_c_idx + self.dimension\n                chip = np_array[:,start_r_idx:end_r_idx,start_c_idx:end_c_idx]\n\n                chip_dict['chip'].append(chip)\n                chip_dict['x'].append(start_r_idx)\n                chip_dict['y'].append(start_c_idx)\n                if chip.mean() == 0 and chip.sum() == 0:\n                    chip_dict['blank'].append('_blank')\n                else:\n                    chip_dict['blank'].append('')\n        return chip_dict\n    \ndef plot_many(pictures, ncols = 4, dpi = 300, is_raster = False):\n    matplotlib.rcParams['figure.dpi'] = dpi\n    nrows = (len(pictures) - 1) // ncols + 1\n    \n    fig,axs = plt.subplots(nrows,ncols,figsize=(10,10))\n    fig.tight_layout()\n    \n     \n    for r,ax in enumerate(axs):\n        for c,row in enumerate(ax):\n            # i is current index in array of axes\n            i = r*ncols + c\n            ax[c].set_title(i)\n            image = pictures[i]\n            # unmaking raster format if necessary\n            if is_raster:\n                image = np.moveaxis(image,0,-1)\n            ax[c].imshow(image);\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets check how it works.","metadata":{}},{"cell_type":"code","source":"сhips_256 = ChipCreator(256, is_raster = True)\nplot_many(сhips_256.make_chips(test_raster_path), is_raster = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_many(сhips_256.make_chips(test_mask), is_raster = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Torch Custom Dataset","metadata":{}},{"cell_type":"code","source":"root_sample_folder =  Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train_sample/sample')\ncsv_sample_path = Path('./output_csvs/untidy_sample.csv')\nroot_train_folder =  Path('../input/spacenet-7-multitemporal-urban-development/SN7_buildings_train/train')\ncsv_train_path = Path('./output_csvs/untidy_train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Сведем в класс датасета еще функции написанные выше (генератор чипсов, вычисление разницы в масках)","metadata":{}},{"cell_type":"code","source":"class CustomSatelliteDataset():\n    \"\"\"SpaceNet7 dataset imagery holder\"\"\"\n    \n    def __init__(self, csv, root_folder, udm = False, transform = None, chip_dim = None):\n        \"\"\"\n        csv: string, path to the csv file with untidy dataframe with all elements adresses\n        root_folder: string, path where imagery is stored\n        udm_use: boolean, condition which specify using of udm masks\n        transform: np.array, transformation matrix if needed\n        chip_dim: int, pixel dimension of the chip size\"\"\"\n        \n        self.csv = pd.read_csv(csv)\n        self.root_folder = root_folder\n        self.udm_use = udm\n        self.transform = transform\n        self.chip_dim = chip_dim\n        \n        \n        if chip_dim is not None:\n            self.сhip_gen = self.__ChipGenerator(dimension = self.chip_dim)\n            # в этом датасете все картинки 1024Х1024\n            self.n_chips = ((1024 - 1) // self.chip_dim + 1)**2\n        \n        self.idx_combinations = self.__get_all_idx_combinations()\n        self.max = self.__len__()\n            \n        \n    def __len__(self):\n        \"\"\"Function that returns the dataset length including chip division\"\"\"\n        if self.chip_dim is not None:\n            return len(self.idx_combinations)*self.n_chips\n        else:\n            return len(self.idx_combinations)\n    \n    def __getitem__(self, idx):\n        \"\"\"Function for read images as required, but not to store them all in memory\"\"\"\n        #checking chip division\n        if self.chip_dim is not None:\n            img_idx = idx//self.n_chips\n            chip_idx = idx%self.n_chips\n        else:\n            img_idx = idx\n        \n        #checking is idx numer or tensor\n        if torch.is_tensor(img_idx):\n            img_index = img_index.to_list()\n        # getting two images    \n        idx1,idx2 = self.idx_combinations[img_idx]\n        # and their path\n        img1_path = self.root_folder/self.csv.loc[idx1,'images_masked']\n        img2_path = self.root_folder/self.csv.loc[idx2,'images_masked']\n        # getting building polygons\n        labels1_path = self.root_folder/self.csv.loc[idx1,'labels_match_pix']\n        labels2_path = self.root_folder/self.csv.loc[idx2,'labels_match_pix']\n        # reading rasters from the paths\n        with rio.open(img1_path) as r1, rio.open(img2_path) as r2:\n            raster1 = r1.read()[0:3]  \n            raster2 = r2.read()[0:3]\n            raster_bounds = r1.bounds\n            rio_transform = r1.transform\n        # to get difference we pass two concatenated images\n        raster_diff = np.concatenate((raster1,raster2),axis=0)\n        # getting  their dates \n        date1 = tuple(self.csv.loc[idx1,['month','year']])\n        date2 = tuple(self.csv.loc[idx2,['month','year']])\n        # read geojson files fwith polygons\n        gdf1 = gpd.read_file(labels1_path).set_index('Id').sort_index()\n        gdf2 = gpd.read_file(labels2_path).set_index('Id').sort_index()\n        \n        # этих пока нет (дописал)\n        # get the change between the 2 satellite images by comparing their polygons\n        gdf_diff = self.__geo_difference(labels1_path,labels2_path)\n        # get the corresponding rasterized image of the geodataframes\n        mask_diff = self.__rasterize_gdf(gdf_diff,out_shape=raster1.shape[1:3])   \n        \n        sample = {'raster1':raster1,'raster2':raster2,'raster_diff':raster_diff,'raster_bounds':raster_bounds,'rio_transform':rio_transform,\n                  'date1':date1,'date2':date2,'mask_diff':mask_diff,'fname':img1_path.parent.parent.stem}\n        \n        if self.transform:\n            sample = self.transform(sample)\n            \n        return sample\n        \n        \n    \n    def  __get_all_idx_combinations(self):\n        \n        all_combinations = []\n        # group by area of interest\n        aoi_groups = self.csv.groupby('aoi')\n        # loop through the groups and get the different index combinations\n        for i,aoi in enumerate(aoi_groups):\n            # get the dataframe in the group\n            loc_frame = aoi[1]\n            # excluding unidentified masks\n            condition = (loc_frame['has_udm'] == False)\n            # return a list of the indices in the location dataframe\n            l = list(loc_frame[condition].index)\n            # use itertools to get all the different combinations between 2 in the list\n            combinations = list(itertools.combinations(l,2))\n            all_combinations.extend(combinations)\n        return all_combinations\n    \n    def __geo_difference(self,geojson1,geojson2):\n        # read geojson into geodataframes\n        gdf1 = gpd.read_file(geojson1).set_index('Id').sort_index()\n        gdf2 = gpd.read_file(geojson2).set_index('Id').sort_index()\n\n        # get geodataframe lengths\n        len_1 = len(gdf1)\n        len_2 = len(gdf2)\n        # check which gdf is longer\n        len_diff = abs(len_2-len_1)\n\n        if len_2 > len_1:\n            start_index = len_2-len_diff\n            diff_gdf = gdf2.iloc[start_index:].copy()\n        else:\n            start_index = len_1-len_diff\n            diff_gdf = gdf1.iloc[start_index:].copy()\n\n        # reset the index\n        diff_gdf.reset_index(inplace=True,drop=True)\n\n        return diff_gdf\n\n    \n    def __rasterize_gdf(self,gdf,out_shape):\n        # if geodataframe is empty return empty mask\n        if len(gdf)==0:\n            return np.zeros((1,*out_shape))\n            \n        mask = features.rasterize(((polygon, 255) for polygon in gdf['geometry']),out_shape=out_shape)\n        \n        return np.expand_dims(mask,axis=0)\n    \n    class __ChipCreator():\n        \"\"\"Class that allows us to split bigger satellite picture and mask into smaller pieces\"\"\"\n        # при попытке воспользоваться попробую посмотреть на форму массива, 4 канала в начале или в конце и нужно ли двигать ось    \n        def __init__(self, dimension):\n            self.dimension = dimension\n            self.chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n        \n        def __read_image(self,image):\n            # checks whether image is a path or array\n            if isinstance(image,(pathlib.PurePath,str)):\n                with Image.open(image) as img:\n                    # converts image into np array\n                    np_array = np.array(img)\n                    return np_array\n            \n            elif isinstance(image,np.ndarray):\n                return image\n            else:\n                raise ValueError(f\"Expected Path or Numpy array received: {type(image)}\")\n        \n        def __call(self, image):\n        \n            #getting image and converting to np.array if necessary\n            np_array = self.__read_image(image)\n            chip_dict = {'chip':[],'x':[],'y':[], 'blank':[]}\n        \n            # then get numbers of chips per row and column\n            n_rows = (np_array.shape[0] - 1) // self.dimension + 1\n            n_cols = (np_array.shape[1] - 1) // self.dimension + 1\n        \n            for r in range(n_rows):\n                for c in range(n_cols):\n                    #starting row and column\n                    start_r_idx = r*self.dimension\n                    end_r_idx = start_r_idx + self.dimension\n                    #ending row and column\n                    start_c_idx = c*self.dimension\n                    end_c_idx = start_c_idx + self.dimension\n                    #cutting fragment by indexes\n                    chip = np_array[:, start_r_idx:end_r_idx,start_c_idx:end_c_idx]\n                    #filling dictionary\n                    chip_dict['chip'].append(chip)\n                    chip_dict['x'].append(start_r_idx)\n                    chip_dict['y'].append(start_c_idx)\n                    \n                    # Marking blank chips\n                    if chip.mean() == 0 and chip.sum() == 0:\n                        chip_dict['blank'].append(1)\n                    else:\n                        chip_dict['blank'].append(0)\n\n            return chip_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetCreator():\n    def __init__(self,chip_dimension=256):\n        self.chip_dimension = chip_dimension\n    \n    def __call__(self,dataset):\n        for d in tqdm(dataset):\n            raster_diff = d['raster_diff']\n            mask_diff = d['mask_diff']\n            \n            self.__fname = d['fname']\n            self.__date1= d['date1']\n            self.__date2 = d['date2']\n            self.__raster_bounds = d['raster_bounds']\n            self.__transform = d['rio_transform']\n            self.__raster_shape = raster_diff.shape[1:3]\n            \n            \n            \n            self.__save_chips(image=raster_diff,subdir_name='chips')\n            self.__save_chips(image=mask_diff,subdir_name='masks')\n            \n            \n    def __save_chips(self,image,subdir_name='chips'):\n        \n        month1,year1 = self.__date1\n        month2,year2 = self.__date2\n        \n        chip_generator = ChipCreator(dimension=self.chip_dimension)\n        chip_dict = chip_generator(image)\n        \n        x_coords = chip_dict['x']\n        y_coords = chip_dict['y']\n        chips = chip_dict['chip']\n        blanks = chip_dict['blank'] \n        \n        im_dir = Path('chip_dataset/change_detection')/Path(self.__fname)/Path(subdir_name)/Path(f'{year1}_{month1}_{year2}_{month2}')\n        im_dir.mkdir(parents=True, exist_ok=True)\n\n        for chip,x,y,blank in zip(chips,x_coords,y_coords,blanks):\n            im_name = f'global_monthly_{year1}_{month1}_{year2}_{month2}_chip_x{x}_y{y}_{self.__fname}{blank}.tif'\n            im_path = im_dir/im_name\n            \n            if subdir_name == 'chips':\n                count = 6\n            else:\n                count = 1\n            \n            # Calculate the new bounds for the raster chips\n            transform = self.__get_geo_transform(x,y)\n            \n            profile = {'driver':'GTiff', 'width':self.chip_dimension,'height':self.chip_dimension,'crs':CRS.from_wkt('LOCAL_CS[\"WGS 84 / Pseudo-Mercator\",UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"3857\"]]'),\n                       'count':count,'dtype':rio.uint8, 'compress':'zip','transform': transform}\n            \n            with rio.open(im_path, 'w',**profile) as dst:\n                dst.write(chip.astype(rio.uint8))\n                \n    \n    def __get_geo_transform(self,x,y):\n        top = self.__raster_bounds[3]\n        bottom = self.__raster_bounds[1]\n        left = self.__raster_bounds[0]\n        right = self.__raster_bounds[2]\n        \n        raster_height = self.__raster_shape[0]\n        raster_width = self.__raster_shape[1]\n        \n        chip_height = (top-bottom)/(raster_height//self.chip_dimension)\n        chip_width = (left-right)/(raster_width//self.chip_dimension)\n        \n        pixel_height = (top-bottom)/raster_height\n        pixel_width = (left-right)/raster_width\n        \n        chip_top = top + y * pixel_height\n        chip_bottom = chip_top + chip_height\n        \n        chip_left = left + x * pixel_width\n        chip_right = chip_left + chip_width\n        \n        bounds = {'left': chip_left, 'bottom': chip_bottom, 'right': chip_right, 'top': chip_top}\n        \n        return rio.Affine(self.__transform[0], self.__transform[1], chip_left,self.__transform[3], self.__transform[4], chip_top)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset = CustomSatelliteDataset(root_folder=root_train_folder,csv=csv_train_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset_creator = DatasetCreator(chip_dimension=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset_creator(dataset=dataset)","metadata":{"_kg_hide-output":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dividing the dataset into chips, according to tqdm's calculations, takes 194 hours. Anyway, after I have learned and reproduced the preparation process, for speed purposes I will take the dataset which is already split into chips the same way. Fortunately it exists and is open on the kaggle.","metadata":{}},{"cell_type":"code","source":"gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Learning preparation\nNow we operate the change detection dataset we prepared above.\nIt is necessary to:\n* redefine paths\n* divide dataset into train/test/valid parts and avoid leakage\n* augment the data without distortion. rotation, reflection, padding, normalisation, convertation to tensor format.\n* make torch dataset with __len__ and __getitem__ attributes, to be able to lazyload and not to store nothing.\n* consider unchanged chips and may be avoid them, make balancing sampler.\n* choose model architecture, learning rate policy, loss and metrics. considering specificity of the task -  Jaccard loss and Intersection over Union (IoU) is most relevant.\n* maintain logging and messaging to tg bot.\n\n\n","metadata":{}},{"cell_type":"code","source":"root_folder = '../input/spacenet-7-change-detection-chips-and-masks/chip_dataset/chip_dataset/change_detection/'\ncsv_path = '../input/spacenet-7-change-detection-chips-and-masks/annotations.csv'\n\nBATCH_SIZE = 64\nNUM_WORKERS = 8","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:33:30.223283Z","iopub.execute_input":"2021-06-02T14:33:30.223832Z","iopub.status.idle":"2021-06-02T14:33:30.231671Z","shell.execute_reply.started":"2021-06-02T14:33:30.223796Z","shell.execute_reply":"2021-06-02T14:33:30.230988Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(csv_path)\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:33:30.235983Z","iopub.execute_input":"2021-06-02T14:33:30.236258Z","iopub.status.idle":"2021-06-02T14:34:04.125245Z","shell.execute_reply.started":"2021-06-02T14:33:30.236229Z","shell.execute_reply":"2021-06-02T14:34:04.124254Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                 chip_path  \\\n1753226  L15-0632E-0892N_2528_4620_13/chips/2018_5_2018...   \n699288   L15-1672E-1207N_6691_3363_13/chips/2018_9_2018...   \n885694   L15-1138E-1216N_4553_3325_13/chips/2018_10_201...   \n3191522  L15-1669E-1153N_6678_3579_13/chips/2018_9_2018...   \n1859227  L15-0924E-1108N_3699_3757_13/chips/2018_1_2018...   \n\n                                                 mask_path  target  \\\n1753226  L15-0632E-0892N_2528_4620_13/masks/2018_5_2018...       0   \n699288   L15-1672E-1207N_6691_3363_13/masks/2018_9_2018...       0   \n885694   L15-1138E-1216N_4553_3325_13/masks/2018_10_201...       0   \n3191522  L15-1669E-1153N_6678_3579_13/masks/2018_9_2018...       0   \n1859227  L15-0924E-1108N_3699_3757_13/masks/2018_1_2018...       0   \n\n                                                     fname        im_dates  \\\n1753226  global_monthly_2018_5_2018_3_chip_x640_y256_L1...   2018_5_2018_3   \n699288   global_monthly_2018_9_2018_11_chip_x832_y640_L...  2018_9_2018_11   \n885694   global_monthly_2018_10_2019_4_chip_x704_y384_L...  2018_10_2019_4   \n3191522  global_monthly_2018_9_2018_2_chip_x768_y448_L1...   2018_9_2018_2   \n1859227  global_monthly_2018_1_2018_2_chip_x384_y192_L1...   2018_1_2018_2   \n\n         year1  month1  year2  month2    x    y                       im_name  \\\n1753226   2018       5   2018       3  640  256  L15-0632E-0892N_2528_4620_13   \n699288    2018       9   2018      11  832  640  L15-1672E-1207N_6691_3363_13   \n885694    2018      10   2019       4  704  384  L15-1138E-1216N_4553_3325_13   \n3191522   2018       9   2018       2  768  448  L15-1669E-1153N_6678_3579_13   \n1859227   2018       1   2018       2  384  192  L15-0924E-1108N_3699_3757_13   \n\n        is_blank  \n1753226    blank  \n699288     blank  \n885694     blank  \n3191522    blank  \n1859227    blank  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>chip_path</th>\n      <th>mask_path</th>\n      <th>target</th>\n      <th>fname</th>\n      <th>im_dates</th>\n      <th>year1</th>\n      <th>month1</th>\n      <th>year2</th>\n      <th>month2</th>\n      <th>x</th>\n      <th>y</th>\n      <th>im_name</th>\n      <th>is_blank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1753226</th>\n      <td>L15-0632E-0892N_2528_4620_13/chips/2018_5_2018...</td>\n      <td>L15-0632E-0892N_2528_4620_13/masks/2018_5_2018...</td>\n      <td>0</td>\n      <td>global_monthly_2018_5_2018_3_chip_x640_y256_L1...</td>\n      <td>2018_5_2018_3</td>\n      <td>2018</td>\n      <td>5</td>\n      <td>2018</td>\n      <td>3</td>\n      <td>640</td>\n      <td>256</td>\n      <td>L15-0632E-0892N_2528_4620_13</td>\n      <td>blank</td>\n    </tr>\n    <tr>\n      <th>699288</th>\n      <td>L15-1672E-1207N_6691_3363_13/chips/2018_9_2018...</td>\n      <td>L15-1672E-1207N_6691_3363_13/masks/2018_9_2018...</td>\n      <td>0</td>\n      <td>global_monthly_2018_9_2018_11_chip_x832_y640_L...</td>\n      <td>2018_9_2018_11</td>\n      <td>2018</td>\n      <td>9</td>\n      <td>2018</td>\n      <td>11</td>\n      <td>832</td>\n      <td>640</td>\n      <td>L15-1672E-1207N_6691_3363_13</td>\n      <td>blank</td>\n    </tr>\n    <tr>\n      <th>885694</th>\n      <td>L15-1138E-1216N_4553_3325_13/chips/2018_10_201...</td>\n      <td>L15-1138E-1216N_4553_3325_13/masks/2018_10_201...</td>\n      <td>0</td>\n      <td>global_monthly_2018_10_2019_4_chip_x704_y384_L...</td>\n      <td>2018_10_2019_4</td>\n      <td>2018</td>\n      <td>10</td>\n      <td>2019</td>\n      <td>4</td>\n      <td>704</td>\n      <td>384</td>\n      <td>L15-1138E-1216N_4553_3325_13</td>\n      <td>blank</td>\n    </tr>\n    <tr>\n      <th>3191522</th>\n      <td>L15-1669E-1153N_6678_3579_13/chips/2018_9_2018...</td>\n      <td>L15-1669E-1153N_6678_3579_13/masks/2018_9_2018...</td>\n      <td>0</td>\n      <td>global_monthly_2018_9_2018_2_chip_x768_y448_L1...</td>\n      <td>2018_9_2018_2</td>\n      <td>2018</td>\n      <td>9</td>\n      <td>2018</td>\n      <td>2</td>\n      <td>768</td>\n      <td>448</td>\n      <td>L15-1669E-1153N_6678_3579_13</td>\n      <td>blank</td>\n    </tr>\n    <tr>\n      <th>1859227</th>\n      <td>L15-0924E-1108N_3699_3757_13/chips/2018_1_2018...</td>\n      <td>L15-0924E-1108N_3699_3757_13/masks/2018_1_2018...</td>\n      <td>0</td>\n      <td>global_monthly_2018_1_2018_2_chip_x384_y192_L1...</td>\n      <td>2018_1_2018_2</td>\n      <td>2018</td>\n      <td>1</td>\n      <td>2018</td>\n      <td>2</td>\n      <td>384</td>\n      <td>192</td>\n      <td>L15-0924E-1108N_3699_3757_13</td>\n      <td>blank</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.target.mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:34:04.127732Z","iopub.execute_input":"2021-06-02T14:34:04.128080Z","iopub.status.idle":"2021-06-02T14:34:04.139998Z","shell.execute_reply.started":"2021-06-02T14:34:04.128043Z","shell.execute_reply":"2021-06-02T14:34:04.139105Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0.17641241530490234"},"metadata":{}}]},{"cell_type":"code","source":"# проверка что таргет = 0 действительно пустые\ndf[df.target == 0].is_blank.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:34:04.141802Z","iopub.execute_input":"2021-06-02T14:34:04.142154Z","iopub.status.idle":"2021-06-02T14:34:05.092320Z","shell.execute_reply.started":"2021-06-02T14:34:04.142116Z","shell.execute_reply":"2021-06-02T14:34:05.091524Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"blank    2644968\nName: is_blank, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"only 17% contains change, we need at least 50%, but probably less or no blank masks.","metadata":{}},{"cell_type":"markdown","source":"## Train/Test/Valid split","metadata":{}},{"cell_type":"code","source":"aoi = df['im_name'].unique()\nlen(aoi)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:34:05.093631Z","iopub.execute_input":"2021-06-02T14:34:05.093962Z","iopub.status.idle":"2021-06-02T14:34:05.368084Z","shell.execute_reply.started":"2021-06-02T14:34:05.093922Z","shell.execute_reply":"2021-06-02T14:34:05.367257Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"60"},"metadata":{}}]},{"cell_type":"markdown","source":"60 unique locations, mentioned as im_name in this df. Lets divide into train/test parts by whole locations to avoid leakage.\n40/10/10","metadata":{}},{"cell_type":"code","source":"train_aoi = aoi[:40]\ntest_aoi = aoi[-20:-10]\nvalid_aoi = aoi[-10:]","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:34:05.369615Z","iopub.execute_input":"2021-06-02T14:34:05.370084Z","iopub.status.idle":"2021-06-02T14:34:05.374800Z","shell.execute_reply.started":"2021-06-02T14:34:05.370045Z","shell.execute_reply":"2021-06-02T14:34:05.374008Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def choose_aoi(df, names):\n    mask = df['im_name'].map(lambda x: x in names)\n    return df[mask].reset_index(drop=True)\n\ndf_dict = {'train' : choose_aoi(df, train_aoi),\n          'test' : choose_aoi(df, test_aoi),\n          'valid' : choose_aoi(df, valid_aoi)\n          }\n\ndel(df)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:34:05.376313Z","iopub.execute_input":"2021-06-02T14:34:05.376638Z","iopub.status.idle":"2021-06-02T14:35:20.236028Z","shell.execute_reply.started":"2021-06-02T14:34:05.376603Z","shell.execute_reply":"2021-06-02T14:35:20.234883Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class TorchDataset(Dataset):\n    \"\"\"Dataset class\n    Args:\n        root_folder: Path object, root directory of picture dataset\n        csv: pandas.DataFrame, untidy df with all data relationships\n        aug: albumentations dictionary\n        preproc: callable, preprocessing function related to specific encoder\n        grayscale: boolean, preprocessing condition to grayscale colored rasters\n    Return:\n        image, mask tensors\"\"\"\n    \n    def __init__(self, root_folder, df, aug = None, preproc = None, grayscale = True):\n        self.root_folder = root_folder\n        self.csv = df\n        self.aug = aug\n        self.preproc = preproc\n        self.grayscale = grayscale\n    \n    def __len__(self):\n        return len(self.csv)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        chip_path = self.root_folder + self.csv.loc[idx,'chip_path']\n        # read chip into numpy array\n        chip = skimage.io.imread(root_folder + self.csv.loc[idx,'chip_path']).astype('float32')\n        if self.grayscale:\n            gray1 = np.dot(chip[:,:,0:3], [0.2989, 0.5870, 0.1140])\n            gray2 = np.dot(chip[:,:,3:], [0.2989, 0.5870, 0.1140])\n            chip = np.divide(np.stack((gray1, gray2),axis = 2),255).astype('float32')\n        # get target for corresponding chip\n        mask = np.abs(np.divide(skimage.io.imread(root_folder + self.csv.loc[idx,'mask_path']),255)).astype('float32')\n        # apply augmentations\n        if self.aug:\n            sample = self.aug(image=chip, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            mask = mask.unsqueeze(0)\n            if self.grayscale:\n                sample = {'I1':image[0,:,:].unsqueeze(0),'I2':image[1,:,:].unsqueeze(0), 'label':mask}\n            else: \n                sample = {'I1':image[0:3,:,:],'I2':image[3:,:,:], 'label':mask}\n            del(image,mask,chip,gray1,gray2)\n            return sample\n        else:\n            image = torch.Tensor(np.moveaxis(chip, 2, 0))\n            mask = torch.Tensor(mask).unsqueeze(0)\n            if self.grayscale:\n                sample = {'I1':image[0,:,:].unsqueeze(0),'I2':image[1,:,:].unsqueeze(0), 'label':mask}\n            else: \n                sample = {'I1':image[0:3,:,:],'I2':image[3:,:,:], 'label':mask}\n            del(mask,chip,gray1,gray2)\n            return sample\n    \n\n    \n    \n\n\nclass BalancedSampler(Sampler):\n    \"\"\"Balancer for torch.DataLoader to adjust chips loading\"\"\"\n    \n    def __init__(self, dataset, percentage = 0.5):\n        \"\"\"\n        dataset: custom torch dataset\n        percentage: float number between 0 and 1, percentage of change containing pictures in batch\n        \"\"\"\n        assert 0 <= percentage <= 1,'percentage must be a value between 0 and 1'\n        \n        self.dataset = dataset\n        self.pct = percentage\n        self.len_ = len(dataset)\n    \n    def __len__(self):\n        return self.len_\n    \n    def __iter__(self):\n        # get indices for chips containing change and blank ones\n        change_chip_idxs = np.where(self.dataset.csv['target'] == 1)[0]\n        blank_chip_idxs = np.where(self.dataset.csv['target'] == 0)[0]\n        # randomly sample from the incides of each class according to percentage value\n        change_chip_idxs = np.random.choice(change_chip_idxs,int(self.len_ * self.pct), replace=True)\n        blank_chip_idxs = np.random.choice(blank_chip_idxs,int(self.len_ * (1 - self.pct))+1, replace=False)\n        # stack and shuffle of sampled indices\n        all_idxs = np.hstack([change_chip_idxs,blank_chip_idxs])\n        np.random.shuffle(all_idxs)\n        return iter(all_idxs)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:20.237694Z","iopub.execute_input":"2021-06-02T14:35:20.238259Z","iopub.status.idle":"2021-06-02T14:35:20.268989Z","shell.execute_reply.started":"2021-06-02T14:35:20.238197Z","shell.execute_reply":"2021-06-02T14:35:20.268111Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Augmentations","metadata":{}},{"cell_type":"code","source":"chip_dimension = 64\naugs = {\n    'train': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        alb.HorizontalFlip(p=0.5),\n        alb.VerticalFlip(p=0.5),\n        ToTensorV2() #apparently doesn't work properly with smp Unet, included in get_preprocessing function\n    ]),\n    'test': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        ToTensorV2()\n    ]), \n    'valid': alb.Compose([\n        alb.PadIfNeeded(min_height=chip_dimension,min_width=chip_dimension,value=0,p=1),\n        ToTensorV2()\n    ]),\n}","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:20.270356Z","iopub.execute_input":"2021-06-02T14:35:20.270959Z","iopub.status.idle":"2021-06-02T14:35:20.294299Z","shell.execute_reply.started":"2021-06-02T14:35:20.270916Z","shell.execute_reply":"2021-06-02T14:35:20.293289Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"augs['train']","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:20.295835Z","iopub.execute_input":"2021-06-02T14:35:20.296422Z","iopub.status.idle":"2021-06-02T14:35:20.311800Z","shell.execute_reply.started":"2021-06-02T14:35:20.296386Z","shell.execute_reply":"2021-06-02T14:35:20.310876Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Compose([\n  PadIfNeeded(always_apply=False, p=1, min_height=64, min_width=64, pad_height_divisor=None, pad_width_divisor=None, border_mode=4, value=0, mask_value=None),\n  HorizontalFlip(always_apply=False, p=0.5),\n  VerticalFlip(always_apply=False, p=0.5),\n  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model setup","metadata":{}},{"cell_type":"markdown","source":"# Metrics evaluation and logging\n## Tg-bot setup","metadata":{}},{"cell_type":"code","source":"import ast\nfile = open(\"../input/logging-utils/credentials.txt\", \"r\")\ncontents = file.read()\ntoken = ast.literal_eval(contents)\nfile.close()\n\ndef telegram_bot_sendtext(bot_message):\n    send_text = 'https://api.telegram.org/bot' + token['bot_token'] + '/sendMessage?chat_id=' + token['bot_chatID'] + '&parse_mode=Markdown&text=' + bot_message\n\n    response = requests.get(send_text)\n\n    return response.json()\n\ndef telegram_send_file (file_address):\n    url = f'https://api.telegram.org/bot' + token['bot_token'] + '/sendVoice'\n    #response = requests.post(url, data=data)\n    post_data = {'chat_id': token['bot_chatID']}\n    with open(file_address, 'r+b') as infile:\n        post_file = {'document': infile}\n        r = requests.post(f'https://api.telegram.org/bot' +token['bot_token'] + '/sendDocument', data=post_data, files=post_file)\n        print(r.text)\n\n\ntest = telegram_bot_sendtext(\"Testing Telegram bot\")\nprint(test)\n\ndef log_traceback(ex, ex_traceback=None):\n    if ex_traceback is None:\n        ex_traceback = ex.__traceback__\n    tb_lines = [ line.rstrip('\\n') for line in\n                 traceback.format_exception(ex.__class__, ex, ex_traceback)]\n    return tb_lines","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:20.313347Z","iopub.execute_input":"2021-06-02T14:35:20.313986Z","iopub.status.idle":"2021-06-02T14:35:21.340753Z","shell.execute_reply.started":"2021-06-02T14:35:20.313917Z","shell.execute_reply":"2021-06-02T14:35:21.339682Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"{'ok': True, 'result': {'message_id': 13287, 'from': {'id': 1759463282, 'is_bot': True, 'first_name': 'my_project_logger', 'username': 'satellite_study_bot'}, 'chat': {'id': 202015929, 'first_name': 'R', 'last_name': 'I', 'username': 'runRudy', 'type': 'private'}, 'date': 1622644521, 'text': 'Testing Telegram bot'}}\n","output_type":"stream"}]},{"cell_type":"code","source":"dic = {'a':'b'}\nwith open('d.json', 'w+') as f:\n    json.dump(dic, f, indent=4) \ntelegram_send_file('./d.json')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:21.342151Z","iopub.execute_input":"2021-06-02T14:35:21.342518Z","iopub.status.idle":"2021-06-02T14:35:22.403011Z","shell.execute_reply.started":"2021-06-02T14:35:21.342478Z","shell.execute_reply":"2021-06-02T14:35:22.402060Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"{\"ok\":true,\"result\":{\"message_id\":13288,\"from\":{\"id\":1759463282,\"is_bot\":true,\"first_name\":\"my_project_logger\",\"username\":\"satellite_study_bot\"},\"chat\":{\"id\":202015929,\"first_name\":\"R\",\"last_name\":\"I\",\"username\":\"runRudy\",\"type\":\"private\"},\"date\":1622644522,\"document\":{\"file_name\":\"d.json\",\"mime_type\":\"application/json\",\"file_id\":\"BQACAgIAAxkDAAIz6GC3lyoofBC_AAERL_mUTmFEgRjiTwACzQwAAgOqwUkjXFaQyOdgJh8E\",\"file_unique_id\":\"AgADzQwAAgOqwUk\",\"file_size\":16}}}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Metrics Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\nclass IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoULoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)/(union + smooth)\n                \n        return 1 - IoU\n\ndef iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n    \"\"\"Fast enough iou calculation function\"\"\"\n    SMOOTH = 1e-6\n    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n    #outputs = outputs.detach()\n    #labels = labels.detach()\n    \n    intersection = (outputs & labels).float().sum((1, 2))\n    union = (outputs | labels).float().sum((1, 2))\n    \n    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n    \n    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n    \n    return thresholded.mean() # to get a batch average\n\ndef segmentation_report(running_preds, running_labels):\n    \"\"\"Function to get a closer look to a confusion metrics and related metrics\"\"\"\n    rp = running_preds.flatten()\n    rl = running_labels.flatten()\n    tn, fp, fn, tp = confusion_matrix(rl, rp, labels=[0,1]).ravel()\n    px_accuracy = (tp+tn) / (tp+fp+tn+fn)\n    precision = tp / (tp+fp)\n    recall = tp / (tp+fn)\n    #calculating intersection over union\n    intersection = np.logical_and(rl, rp)\n    union = np.logical_or(rl, rp)\n    iou_score = np.sum(intersection) / np.sum(union)\n    fmeasure = 2 * precision * recall / (precision + recall)\n    #making report\n    report = { 'tp/tn/fp/fn' : (tp,tn,fp,fn),\n              'px_accuracy': px_accuracy,\n              'precision': precision,\n              'recall': recall,\n              'iou_score': iou_score,\n              'fmeasure':fmeasure\n             }\n    return report\n\n\ndef log_batch_statistics(batch_number,batch_labels,batch_preds,phase,loss,since,num_batches,period=500):\n    if batch_number % period == 0:\n        iou_score = segmentation_report(batch_preds,batch_labels)\n        time_elapsed = time.time() - since\n\n        if phase == 'train':\n            telegram_bot_sendtext('TRAINING BATCH')\n        else:\n            telegram_bot_sendtext('VALIDATION BATCH')\n            \n        telegram_bot_sendtext('-'*50)\n        telegram_bot_sendtext(f'\\n{batch_number}/{num_batches-1}:')\n        telegram_bot_sendtext(f'Total Time Elapsed: {time_elapsed/60:.2f} mins')\n        telegram_bot_sendtext(f'Batch Loss: {loss.item():.4f}\\n')\n        telegram_bot_sendtext(f\"``` IoU_score:{iou_score}\\n ```\")\n        telegram_bot_sendtext('-'*50)\n\ndef break_time_limit(start_time,time_limit=28080):\n    time_elapsed = time()-start_time\n    if time_elapsed > time_limit:\n        sys.exit()","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:22.404766Z","iopub.execute_input":"2021-06-02T14:35:22.405116Z","iopub.status.idle":"2021-06-02T14:35:22.562875Z","shell.execute_reply.started":"2021-06-02T14:35:22.405080Z","shell.execute_reply":"2021-06-02T14:35:22.562221Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Model Setup","metadata":{}},{"cell_type":"code","source":"#learning policy params\ngrayscale = True\nif grayscale == True:\n    in_channels = 2\nelse: in_channels = 6\n\nN_EPOCHS = 25\n\n# turning on GPU if possible\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nprint()\n\n# cleaning GPU\ngc.collect() \ntorch.cuda.empty_cache()\ntorch.backends.cudnn.benchmark = True\n\n# mean percentage of positives is 6.5% from the frame, median is 3.4%, so weights for bce loss are required.\nweights = torch.Tensor([28]).to(device) \n\n#criterion = torch.nn.BCEWithLogitsLoss(pos_weight = weights) \ncriterion = IoULoss()\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:22.564103Z","iopub.execute_input":"2021-06-02T14:35:22.564507Z","iopub.status.idle":"2021-06-02T14:35:26.573130Z","shell.execute_reply.started":"2021-06-02T14:35:22.564468Z","shell.execute_reply":"2021-06-02T14:35:26.572008Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Using device: cuda:0\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Change detection FC model","metadata":{}},{"cell_type":"markdown","source":"I decided to relate on a following articale: Daudt, R.C., Le Saux, B. and Boulch, A., 2018, October. Fully convolutional siamese networks for change detection. In 2018 25th IEEE International Conference on Image Processing (ICIP) (pp. 4063-4067). IEEE.https://rcdaudt.github.io/files/2018icip-fully-convolutional.pdf .  \nIt desctibes neural networks architectures for CD specifically, which authors consider as most effective in cases of training from zero, without transfer learning or fine tuning of pretrained models.  \nThese are:\n* FC-EF (Fully Convolutional - Early Fusion)\n* FC_Siam_conc\n* FC_Siam_diff  \n\n\nThe first one involves feeding two of the concatenated rasters and their joint processing, the second and third involves separate treatment of the raster inputs at the beginning, during the convolutional stage, with identical branches with the shared weights and bias. Both outputs of individual branches are concatenated and fed to the input of a single network, ending with a layer of softmax or sigmoid transformation. Siam_conc and Siam_diff differ from each other in the mechanism of implementation of the feature skipping, in the first case it is more intuitive - intermediate features from the two branches of the conv stage are concatenated with the output at the respective upconv stages. In the case of Siam_diff, instead of two sets, the absolute difference between the intermediate features is concatenated. In the conclusion to the article it is proposed to use FC_Siam_diff as the most productive approach (although it is noted that all the above-mentioned methods are superior to the use of, for example, Transfer Learning) the next most productive one is Early Fusion.","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass Unet(nn.Module):\n    \"\"\"EF segmentation network.\"\"\"\n\n    def __init__(self, input_nbr, label_nbr):\n        super(Unet, self).__init__()\n\n        self.input_nbr = input_nbr\n\n        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(16)\n        self.do11 = nn.Dropout2d(p=0.2)\n        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(16)\n        self.do12 = nn.Dropout2d(p=0.2)\n\n        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(32)\n        self.do21 = nn.Dropout2d(p=0.2)\n        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(32)\n        self.do22 = nn.Dropout2d(p=0.2)\n\n        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(64)\n        self.do31 = nn.Dropout2d(p=0.2)\n        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(64)\n        self.do32 = nn.Dropout2d(p=0.2)\n        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(64)\n        self.do33 = nn.Dropout2d(p=0.2)\n\n        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(128)\n        self.do41 = nn.Dropout2d(p=0.2)\n        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(128)\n        self.do42 = nn.Dropout2d(p=0.2)\n        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(128)\n        self.do43 = nn.Dropout2d(p=0.2)\n\n\n        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(128)\n        self.do43d = nn.Dropout2d(p=0.2)\n        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(128)\n        self.do42d = nn.Dropout2d(p=0.2)\n        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(64)\n        self.do41d = nn.Dropout2d(p=0.2)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(64)\n        self.do33d = nn.Dropout2d(p=0.2)\n        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(64)\n        self.do32d = nn.Dropout2d(p=0.2)\n        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(32)\n        self.do31d = nn.Dropout2d(p=0.2)\n\n        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(32)\n        self.do22d = nn.Dropout2d(p=0.2)\n        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(16)\n        self.do21d = nn.Dropout2d(p=0.2)\n\n        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(16)\n        self.do12d = nn.Dropout2d(p=0.2)\n        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n\n        self.sm = nn.Sigmoid()\n\n    def forward(self, x1, x2):\n\n        x = torch.cat((x1, x2), 1)\n\n        \"\"\"Forward method.\"\"\"\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x))))\n        x12 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12, kernel_size=2, stride=2)\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43, kernel_size=2, stride=2)\n\n\n        # Stage 4d\n        x4d = self.upconv4(x4p)\n        pad4 = ReplicationPad2d((0, x43.size(3) - x4d.size(3), 0, x43.size(2) - x4d.size(2)))\n        x4d = torch.cat((pad4(x4d), x43), 1)\n        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n\n        # Stage 3d\n        x3d = self.upconv3(x41d)\n        pad3 = ReplicationPad2d((0, x33.size(3) - x3d.size(3), 0, x33.size(2) - x3d.size(2)))\n        x3d = torch.cat((pad3(x3d), x33), 1)\n        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n\n        # Stage 2d\n        x2d = self.upconv2(x31d)\n        pad2 = ReplicationPad2d((0, x22.size(3) - x2d.size(3), 0, x22.size(2) - x2d.size(2)))\n        x2d = torch.cat((pad2(x2d), x22), 1)\n        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n\n        # Stage 1d\n        x1d = self.upconv1(x21d)\n        pad1 = ReplicationPad2d((0, x12.size(3) - x1d.size(3), 0, x12.size(2) - x1d.size(2)))\n        x1d = torch.cat((pad1(x1d), x12), 1)\n        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n        x11d = self.conv11d(x12d)\n\n        return self.sm(x11d)\n\nclass SiamUnet_diff(nn.Module):\n    \"\"\"SiamUnet_diff segmentation network.\"\"\"\n\n    def __init__(self, input_nbr, label_nbr):\n        super(SiamUnet_diff, self).__init__()\n\n        self.input_nbr = input_nbr\n\n        self.conv11 = nn.Conv2d(input_nbr, 16, kernel_size=3, padding=1)\n        self.bn11 = nn.BatchNorm2d(16)\n        self.do11 = nn.Dropout2d(p=0.2)\n        self.conv12 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n        self.bn12 = nn.BatchNorm2d(16)\n        self.do12 = nn.Dropout2d(p=0.2)\n\n        self.conv21 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.bn21 = nn.BatchNorm2d(32)\n        self.do21 = nn.Dropout2d(p=0.2)\n        self.conv22 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n        self.bn22 = nn.BatchNorm2d(32)\n        self.do22 = nn.Dropout2d(p=0.2)\n\n        self.conv31 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn31 = nn.BatchNorm2d(64)\n        self.do31 = nn.Dropout2d(p=0.2)\n        self.conv32 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn32 = nn.BatchNorm2d(64)\n        self.do32 = nn.Dropout2d(p=0.2)\n        self.conv33 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.bn33 = nn.BatchNorm2d(64)\n        self.do33 = nn.Dropout2d(p=0.2)\n\n        self.conv41 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn41 = nn.BatchNorm2d(128)\n        self.do41 = nn.Dropout2d(p=0.2)\n        self.conv42 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn42 = nn.BatchNorm2d(128)\n        self.do42 = nn.Dropout2d(p=0.2)\n        self.conv43 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.bn43 = nn.BatchNorm2d(128)\n        self.do43 = nn.Dropout2d(p=0.2)\n\n        self.upconv4 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv43d = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n        self.bn43d = nn.BatchNorm2d(128)\n        self.do43d = nn.Dropout2d(p=0.2)\n        self.conv42d = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n        self.bn42d = nn.BatchNorm2d(128)\n        self.do42d = nn.Dropout2d(p=0.2)\n        self.conv41d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn41d = nn.BatchNorm2d(64)\n        self.do41d = nn.Dropout2d(p=0.2)\n\n        self.upconv3 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv33d = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n        self.bn33d = nn.BatchNorm2d(64)\n        self.do33d = nn.Dropout2d(p=0.2)\n        self.conv32d = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n        self.bn32d = nn.BatchNorm2d(64)\n        self.do32d = nn.Dropout2d(p=0.2)\n        self.conv31d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn31d = nn.BatchNorm2d(32)\n        self.do31d = nn.Dropout2d(p=0.2)\n\n        self.upconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv22d = nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1)\n        self.bn22d = nn.BatchNorm2d(32)\n        self.do22d = nn.Dropout2d(p=0.2)\n        self.conv21d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn21d = nn.BatchNorm2d(16)\n        self.do21d = nn.Dropout2d(p=0.2)\n\n        self.upconv1 = nn.ConvTranspose2d(16, 16, kernel_size=3, padding=1, stride=2, output_padding=1)\n\n        self.conv12d = nn.ConvTranspose2d(32, 16, kernel_size=3, padding=1)\n        self.bn12d = nn.BatchNorm2d(16)\n        self.do12d = nn.Dropout2d(p=0.2)\n        self.conv11d = nn.ConvTranspose2d(16, label_nbr, kernel_size=3, padding=1)\n\n        self.sm = nn.Sigmoid()\n\n    def forward(self, x1, x2):\n\n\n        \"\"\"Forward method.\"\"\"\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x1))))\n        x12_1 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12_1, kernel_size=2, stride=2)\n\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22_1 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22_1, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33_1 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33_1, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43_1 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43_1, kernel_size=2, stride=2)\n\n        ####################################################\n        # Stage 1\n        x11 = self.do11(F.relu(self.bn11(self.conv11(x2))))\n        x12_2 = self.do12(F.relu(self.bn12(self.conv12(x11))))\n        x1p = F.max_pool2d(x12_2, kernel_size=2, stride=2)\n\n\n        # Stage 2\n        x21 = self.do21(F.relu(self.bn21(self.conv21(x1p))))\n        x22_2 = self.do22(F.relu(self.bn22(self.conv22(x21))))\n        x2p = F.max_pool2d(x22_2, kernel_size=2, stride=2)\n\n        # Stage 3\n        x31 = self.do31(F.relu(self.bn31(self.conv31(x2p))))\n        x32 = self.do32(F.relu(self.bn32(self.conv32(x31))))\n        x33_2 = self.do33(F.relu(self.bn33(self.conv33(x32))))\n        x3p = F.max_pool2d(x33_2, kernel_size=2, stride=2)\n\n        # Stage 4\n        x41 = self.do41(F.relu(self.bn41(self.conv41(x3p))))\n        x42 = self.do42(F.relu(self.bn42(self.conv42(x41))))\n        x43_2 = self.do43(F.relu(self.bn43(self.conv43(x42))))\n        x4p = F.max_pool2d(x43_2, kernel_size=2, stride=2)\n\n\n\n        # Stage 4d\n        x4d = self.upconv4(x4p)\n        pad4 = ReplicationPad2d((0, x43_1.size(3) - x4d.size(3), 0, x43_1.size(2) - x4d.size(2)))\n        x4d = torch.cat((pad4(x4d), torch.abs(x43_1 - x43_2)), 1)\n        x43d = self.do43d(F.relu(self.bn43d(self.conv43d(x4d))))\n        x42d = self.do42d(F.relu(self.bn42d(self.conv42d(x43d))))\n        x41d = self.do41d(F.relu(self.bn41d(self.conv41d(x42d))))\n\n        # Stage 3d\n        x3d = self.upconv3(x41d)\n        pad3 = ReplicationPad2d((0, x33_1.size(3) - x3d.size(3), 0, x33_1.size(2) - x3d.size(2)))\n        x3d = torch.cat((pad3(x3d), torch.abs(x33_1 - x33_2)), 1)\n        x33d = self.do33d(F.relu(self.bn33d(self.conv33d(x3d))))\n        x32d = self.do32d(F.relu(self.bn32d(self.conv32d(x33d))))\n        x31d = self.do31d(F.relu(self.bn31d(self.conv31d(x32d))))\n\n        # Stage 2d\n        x2d = self.upconv2(x31d)\n        pad2 = ReplicationPad2d((0, x22_1.size(3) - x2d.size(3), 0, x22_1.size(2) - x2d.size(2)))\n        x2d = torch.cat((pad2(x2d), torch.abs(x22_1 - x22_2)), 1)\n        x22d = self.do22d(F.relu(self.bn22d(self.conv22d(x2d))))\n        x21d = self.do21d(F.relu(self.bn21d(self.conv21d(x22d))))\n\n        # Stage 1d\n        x1d = self.upconv1(x21d)\n        pad1 = ReplicationPad2d((0, x12_1.size(3) - x1d.size(3), 0, x12_1.size(2) - x1d.size(2)))\n        x1d = torch.cat((pad1(x1d), torch.abs(x12_1 - x12_2)), 1)\n        x12d = self.do12d(F.relu(self.bn12d(self.conv12d(x1d))))\n        x11d = self.conv11d(x12d)\n\n        return self.sm(x11d)\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:26.574695Z","iopub.execute_input":"2021-06-02T14:35:26.575280Z","iopub.status.idle":"2021-06-02T14:35:26.686311Z","shell.execute_reply.started":"2021-06-02T14:35:26.575241Z","shell.execute_reply":"2021-06-02T14:35:26.685525Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"!pip install segmentation_models_pytorch\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:08:36.774878Z","iopub.execute_input":"2021-06-02T15:08:36.775246Z","iopub.status.idle":"2021-06-02T15:08:42.648488Z","shell.execute_reply.started":"2021-06-02T15:08:36.775195Z","shell.execute_reply":"2021-06-02T15:08:42.647638Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Requirement already satisfied: segmentation_models_pytorch in /opt/conda/lib/python3.7/site-packages (0.1.3)\nRequirement already satisfied: timm==0.3.2 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.3.2)\nRequirement already satisfied: efficientnet-pytorch==0.6.3 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.6.3)\nRequirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.8.1)\nRequirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.7.4)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.7.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.55.1)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (0.18.2)\nRequirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (3.7.4.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (0.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.19.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (7.2.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"if grayscale == True:\n    net, net_name = smp.Unet('resnet34', in_channels = 2, activation='sigmoid'), 'FC-EF'\n    #net, net_name = SiamUnet_diff(1,1), 'FC-Siam-diff'\n    #net, net_name = SiamUnet_conc(1,1), 'FC-Siam-conc'\n    #net, net_name = Unet(2,1), 'FC-EF'\n    \nelse:\n    net, net_name = smp.Unet('resnet34', in_channels = 6, activation='sigmoid'), 'FC-EF'\n    #net, net_name = SiamUnet_diff(3, 1), 'FC-Siam-diff'\n    #net, net_name = SiamUnet_conc(3,1), 'FC-Siam-conc'\n    #net, net_name = Unet(3*2,1), 'FC-EF'\nnet = net.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:14:37.734096Z","iopub.execute_input":"2021-06-02T15:14:37.734461Z","iopub.status.idle":"2021-06-02T15:14:47.852271Z","shell.execute_reply.started":"2021-06-02T15:14:37.734429Z","shell.execute_reply":"2021-06-02T15:14:47.851350Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/83.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df4eeb6c33c246aa8d513732241824ea"}},"metadata":{}}]},{"cell_type":"code","source":"#defining datasets, samplers and dataloaders\ndatasets = {x:TorchDataset(root_folder = root_folder,df = df_dict[x],aug = None, preproc = None, grayscale = grayscale) for x in ['train','test','valid']}\n\nsamplers = {'train':BalancedSampler(datasets['train'], percentage = 1), \n            'test':BalancedSampler(datasets['test'], percentage = 1),\n            'valid':BalancedSampler(datasets['valid'], percentage = 1),\n            'sanity':BalancedSampler(datasets['train'], percentage = 1)}\n\ndataloaders = {x: DataLoader(dataset=datasets[x],\n                             batch_size=BATCH_SIZE,\n                             sampler=samplers[x],\n                             num_workers=16) for x in ['train','test','valid']}\n\ndataset_sizes = {x: len(datasets[x]) for x in ['train', 'test', 'valid']}\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:20:06.961505Z","iopub.execute_input":"2021-06-02T15:20:06.961824Z","iopub.status.idle":"2021-06-02T15:20:06.969060Z","shell.execute_reply.started":"2021-06-02T15:20:06.961793Z","shell.execute_reply":"2021-06-02T15:20:06.968072Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Train function","metadata":{}},{"cell_type":"code","source":"net = torch.load('../input/satellite-models/netFC-Siam-diff-best_epoch-1_iou_score-0.023448613236376363.tar')","metadata":{"execution":{"iopub.status.busy":"2021-06-02T14:35:26.761752Z","iopub.execute_input":"2021-06-02T14:35:26.762409Z","iopub.status.idle":"2021-06-02T14:35:27.028505Z","shell.execute_reply.started":"2021-06-02T14:35:26.762371Z","shell.execute_reply":"2021-06-02T14:35:27.027731Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def train(net, n_epochs = 25):\n    print('epoch,train_loss,train_iou,test_loss,test_iou',file=open('loss_log.txt', 'a'))\n    start_time = time()\n    # telegram_bot_sendtext(f'Training started')\n    scaler = GradScaler()\n        \n    iou = -1\n    best_iou = -1\n    \n    lss = 100000000\n    best_lss = 100000000\n    \n    train_epoch_iou = 0\n    test_epoch_iou = 0\n    \n    \n    #defining optimizer and scheduler\n    optimizer = torch.optim.Adam(net.parameters(), lr = 0.005)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n    \n\n    for epoch_index in tqdm(range(n_epochs)):\n        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(n_epochs))\n        train_running_loss = 0.0\n        train_running_iou = 0.0\n        test_running_loss = 0.0\n        test_running_iou = 0.0\n        \n        for phase in ['train','test']:\n            if phase == 'train':\n                net.train()  # Set model to training mode\n            else:\n                net.eval()   # Set model to evaluate mode\n\n            num_batches = len(dataloaders[phase])\n            for batch_index, batch in enumerate(tqdm(dataloaders[phase])):\n                torch.cuda.empty_cache()\n                I1 = Variable(batch['I1'].float().to(device))\n                I2 = Variable(batch['I2'].float().to(device))\n                labels = Variable(batch['label'].float().to(device))\n\n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    with autocast():\n                        # outputs = net(I1, I2)\n                        outputs = net(torch.stack((I1, I2), axis = 1).squeeze()) # fusion for smp.Unet\n                        loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs.data, 1)\n                    del(_,outputs)\n                    if phase == 'train':\n                        scaler.scale(loss).backward()\n                        scaler.step(optimizer)\n                        optimizer.step()\n                        scaler.update()\n                    del(I1, I2)\n#                     except Exception as e:\n#                             msg = log_traceback(e)\n#                             telegram_bot_sendtext(f'failed at batch {batch_index}, with message: {msg}')\n#                             raise e\n#                             break\n#                     log_batch_statistics(batch_index,labels.data.long(),outputs.data.long(),phase,loss=loss,num_batches=num_batches,since=start_time)\n                if phase == 'train':\n                    train_running_loss += loss.item()\n                    # train_running_iou += iou_pytorch(preds.int().to('cpu'), labels.int().to('cpu')).item()\n                else:\n                    test_running_loss += loss.item()\n                    # test_running_iou += iou_pytorch(preds.int().to('cpu'), labels.int().to('cpu')).item()\n                                \n            if phase == 'train':\n                scheduler.step()\n                train_epoch_loss = train_running_loss / dataset_sizes[phase] * BATCH_SIZE\n                # train_epoch_iou = train_running_iou / dataset_sizes[phase] * BATCH_SIZE\n            else:\n                test_epoch_loss = test_running_loss / dataset_sizes[phase] * BATCH_SIZE\n                # test_epoch_iou = test_running_iou / dataset_sizes[phase] * BATCH_SIZE\n            print(f'{epoch_index},{train_epoch_loss}, {train_epoch_iou},{test_epoch_loss},{test_epoch_iou}', file=open('loss_log.txt', 'a'))\n#             print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=20), file=open(\"profiler.txt\", \"a\"))\n#             telegram_send_file('./profiler.txt')\n            \n#             if epoch_iou > best_iou:\n#                 best_iou = epoch_iou\n#                 save_str = f'./net{net_name}-best_epoch-' + str(epoch_index + 1) + '_iou_score-' + str(best_iou) + '.pth.tar'\n#                 torch.save(net, save_str)\n#                 telegram_send_file(save_str)\n#                 print('Model saved!')\n        \n            if (phase  == 'train') and (train_epoch_loss < best_lss):\n                best_lss = train_epoch_loss\n                save_str = f'./net{net_name}-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(best_lss) + '.pth.tar'\n                torch.save(net, save_str)\n                telegram_send_file(save_str)\n                print('Model saved!')\n            \n            if (phase == 'test') and (test_epoch_loss < best_lss):\n                best_lss = test_epoch_loss\n                save_str = f'./net{net_name}-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(best_lss) + '.pth.tar'\n                torch.save(net, save_str)\n                telegram_send_file(save_str)\n                print('Model saved!')\n        \n    \n    model_str = './model.pth.tar'\n    torch.save(net, model_str)\n    telegram_send_file(model_str)\n    print('Model saved!')\n    time_elapsed = time() - start_time\n    print()\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val change_accuracy: {best_fm:4f}')\n    \n    return net\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:20:08.484840Z","iopub.execute_input":"2021-06-02T15:20:08.485151Z","iopub.status.idle":"2021-06-02T15:20:08.502508Z","shell.execute_reply.started":"2021-06-02T15:20:08.485121Z","shell.execute_reply":"2021-06-02T15:20:08.501302Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model = train(net,2)  ","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:20:09.415692Z","iopub.execute_input":"2021-06-02T15:20:09.416025Z","iopub.status.idle":"2021-06-02T16:20:11.104315Z","shell.execute_reply.started":"2021-06-02T15:20:09.415993Z","shell.execute_reply":"2021-06-02T16:20:11.102174Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92897fc0a8aa442bad87a6518d87168c"}},"metadata":{}},{"name":"stdout","text":"Epoch: 1 of 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/35008 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d3b1f1a907414ca4d194ce87b56b97"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-49-a833e4d689f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-48-2a8d776da1c2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, n_epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"dataset_sizes['train']","metadata":{"execution":{"iopub.status.busy":"2021-06-01T08:17:47.371495Z","iopub.status.idle":"2021-06-01T08:17:47.372336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_sizes['valid']","metadata":{"execution":{"iopub.status.busy":"2021-06-01T08:17:47.373493Z","iopub.status.idle":"2021-06-01T08:17:47.374219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results visualization","metadata":{}},{"cell_type":"code","source":"model_path = '../input/satellite-models/netFC-Siam-diff-best_epoch-1_iou_score-0.825063088258398.pth.tar'\nmodel = torch.load(model_path)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T08:58:57.911174Z","iopub.execute_input":"2021-05-25T08:58:57.912066Z","iopub.status.idle":"2021-05-25T08:58:58.153508Z","shell.execute_reply.started":"2021-05-25T08:58:57.912018Z","shell.execute_reply":"2021-05-25T08:58:58.152448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.cuda()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T08:58:58.155174Z","iopub.execute_input":"2021-05-25T08:58:58.15562Z","iopub.status.idle":"2021-05-25T08:58:58.168376Z","shell.execute_reply.started":"2021-05-25T08:58:58.155574Z","shell.execute_reply":"2021-05-25T08:58:58.166619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2021-05-25T08:58:58.171034Z","iopub.execute_input":"2021-05-25T08:58:58.171645Z","iopub.status.idle":"2021-05-25T08:58:58.182248Z","shell.execute_reply.started":"2021-05-25T08:58:58.17157Z","shell.execute_reply":"2021-05-25T08:58:58.180687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_model(model):\n    iou_list = []\n    for batch_index, batch in enumerate(tqdm(dataloaders['valid'])):\n        I1 = Variable(batch['I1'].float().to(device))\n        I2 = Variable(batch['I2'].float().to(device))\n        labels = Variable(batch['label'].int().to(device))\n        outputs = model(I1,I2)\n        iou_list.append(iou_pytorch(outputs.int().to('cpu'), labels.to('cpu')))\n    print(np.mean(iou_list)) ","metadata":{"execution":{"iopub.status.busy":"2021-05-25T08:58:58.183484Z","iopub.execute_input":"2021-05-25T08:58:58.183919Z","iopub.status.idle":"2021-05-25T08:58:58.197534Z","shell.execute_reply.started":"2021-05-25T08:58:58.183873Z","shell.execute_reply":"2021-05-25T08:58:58.195792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_model(model)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T08:58:58.203077Z","iopub.execute_input":"2021-05-25T08:58:58.203424Z","iopub.status.idle":"2021-05-25T09:48:09.996394Z","shell.execute_reply.started":"2021-05-25T08:58:58.203393Z","shell.execute_reply":"2021-05-25T09:48:09.995056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(dataloaders['valid']))\nn = np.random.randint(BATCH_SIZE)\n_, preds = torch.max(model(batch['I1'].cuda(),batch['I2'].cuda()).data,1)","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:26:13.320763Z","iopub.execute_input":"2021-05-25T10:26:13.324393Z","iopub.status.idle":"2021-05-25T10:26:19.130189Z","shell.execute_reply.started":"2021-05-25T10:26:13.324335Z","shell.execute_reply":"2021-05-25T10:26:19.128947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,4)\nfig.suptitle('Random chip prediction')\nfig.tight_layout() \naxs[0].imshow(batch['I1'][n].squeeze())\naxs[1].imshow(batch['I2'][n].squeeze())\naxs[2].imshow(batch['label'][n].squeeze())\naxs[3].imshow(preds[n].cpu())\n","metadata":{"execution":{"iopub.status.busy":"2021-05-25T10:26:19.132676Z","iopub.execute_input":"2021-05-25T10:26:19.133177Z","iopub.status.idle":"2021-05-25T10:26:19.904441Z","shell.execute_reply.started":"2021-05-25T10:26:19.133131Z","shell.execute_reply":"2021-05-25T10:26:19.903265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sandbox for debuging","metadata":{}},{"cell_type":"code","source":"torch.stack((datasets['train'][0]['I1'],datasets['train'][0]['I1']),axis = 1).squeeze().shape","metadata":{"execution":{"iopub.status.busy":"2021-06-02T15:17:46.261035Z","iopub.execute_input":"2021-06-02T15:17:46.261371Z","iopub.status.idle":"2021-06-02T15:17:46.276772Z","shell.execute_reply.started":"2021-06-02T15:17:46.261340Z","shell.execute_reply":"2021-06-02T15:17:46.275894Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"torch.Size([2, 64, 64])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}