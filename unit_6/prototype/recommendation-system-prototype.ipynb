{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1. Global settings"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom datetime import datetime, timedelta\nimport json\n\nimport pandas_profiling\nimport nltk\n\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import wordnet, stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy import sparse \n\nimport pickle\n\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\n\nfrom lightfm import LightFM\nfrom lightfm.cross_validation import random_train_test_split\nfrom lightfm.evaluation import auc_score, precision_at_k, recall_at_k\nimport sklearn\nfrom sklearn.model_selection import train_test_split\n","execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Global settings"},{"metadata":{"trusted":true},"cell_type":"code","source":"COL_USER = \"userid\"\nCOL_ITEM = \"itemid\"\nCOL_RATING = \"overall\"\nCOL_PREDICTION = \"rating\"\nCOL_TIMESTAMP = \"timestamp\"\nstop_words = stopwords.words(\"english\")\n\nLR = 0.07\nLOSS_FUNCTION = 'logistic'\nLEARNING_SCHEDULE = 'adagrad'\nRANDOM_STATE = 42\n\nNUM_THREADS = 4 #число потоков\nNUM_COMPONENTS = 30 #число параметров вектора \nNUM_EPOCHS = 20 #число эпох обучения\n","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# 2. DATA"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Data downloading"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.read_csv('../input/recommendationsv4/train.csv')\ntrain = pd.read_csv('../input/processed-data/train_processed(2).csv')\ntest = pd.read_csv('../input/recommendationsv4/test.csv')\nsubmission = pd.read_csv('/kaggle/input/recommendationsv4/sample_submission.csv')\n\n# reading metadata json\nwith open('/kaggle/input/recommendationsv4/meta_Grocery_and_Gourmet_Food.json') as f:\n    meta_list = []\n    for line in f.readlines():\n        meta_list.append(json.loads(line))\n        \nmeta = pd.DataFrame(meta_list)\n\n# dropping duplicates\ntrain.drop_duplicates(inplace = True)\n\n# merging train and meta on asin column (Amazon Standard Identification Number)\ntrain = pd.merge(train, meta, on='asin')\ntest = pd.merge(test, meta, on='asin')","execution_count":3,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.to_csv('train.csv') #checkpoint\n# test.to_csv('test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(meta) #to leave some RAM","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Data understanding"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## 2.3 Data transformation\n### 2.3.1 Simple cleaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_timestamp(df):\n    \"\"\"converting unixtime to datetime\"\"\"\n    return df.apply(\n    lambda x:  datetime.utcfromtimestamp(x['unixReviewTime']).strftime('%Y-%m-%d'), axis = 1)\ntrain[COL_TIMESTAMP] = get_timestamp(train)\ntest[COL_TIMESTAMP] = get_timestamp(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.main_cat = train.main_cat.fillna('Other') #filling empty categories with \"other\"\ntest.main_cat = test.main_cat.fillna('Other')\n\ndic_verified = {\n    True: 1,\n    False: 0\n}\ntrain['verified'] = train['verified'].map(dic_verified) #replacing \"verified\"-feature with ints\ntest['verified'] = test['verified'].map(dic_verified)","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 Brief feature engineering"},{"metadata":{},"cell_type":"markdown","source":"For faster execution and to not concentrate too much on feature processing, I'll make features only from review summaries and titles (because it may contain such information as \"non-gluten\", \"halal\", \"vegetarian\", brands, specific product types references and so on."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nstemmer = PorterStemmer()\n\ndef get_sentence(sentence, cond = \"not\"):\n    \"\"\"tokenizing sentences in review and deleting stopwords\"\"\"\n    if cond ==\"stem\":\n        words = nltk.word_tokenize(sentence)\n        without_stop_words = [stemmer.stem(word) for word in words if not word in stop_words]\n    else: without_stop_words = [word for word in sentence.split(' ') if not word in stop_words]\n    return ' '.join(without_stop_words)\n\ndef get_features(series):\n    vectorizer = CountVectorizer(min_df = 0.05) # I wanted to try tfidf, but these amount of RAM is quiet limited to have such\n    list_rvw = series.fillna('noreview') # a variety of types\n    values = vectorizer.fit_transform(list_rvw)\n\n    # Get the features as a pandas DataFrame\n    feature_names = vectorizer.get_feature_names()\n    return pd.DataFrame(values.toarray(), columns = feature_names)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's possible to lemmatize features, but I won’t do it for now because lemmatization on this dataset would be a time-consuming procedure. Stemming would be a better option."},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting lemmatized summary sentences and tokenized titles\n#train['summary'] = train['summary'].progress_apply(lambda x: str(get_sentence(x, \"stem\")) if type(x) == str else x)\ntrain['token_title'] = train['title'].progress_apply(lambda x: str(get_sentence(x)) if type(x) == str else x)","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=876561.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4452b8e7f7224ff8bef5104560727ba4"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extruding feature from summaries and titles\nsummary_features = get_features(train['summary'])\nsummary_features['itemid'] = train['itemid']\ntitle_features = get_features(train['token_title'])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_features['itemid'] = train['itemid']","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lst = ['overall', 'verified', 'unixReviewTime',\n       'vote',  'userid', 'itemid',\n       'rating', 'main_cat']\ndf_train = train.loc[:,lst]","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(train)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#summary_features.reset_index(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#summary_features.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"title_features = title_features.loc[:,'bag':'itemid']\ntitle_features","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"        bag  bags  box  chocolate  coffee  count  dark  free  gluten  no  \\\n0         0     0    0          0       0      0     0     0       0   0   \n1         0     0    0          0       0      0     0     0       0   0   \n2         0     0    0          0       0      0     0     0       0   0   \n3         0     0    0          0       0      0     0     0       0   0   \n4         0     0    0          0       0      0     0     0       0   0   \n...     ...   ...  ...        ...     ...    ...   ...   ...     ...  ..   \n876556    0     0    0          1       0      0     0     0       0   0   \n876557    0     0    0          0       0      0     0     1       0   0   \n876558    0     0    0          0       1      0     0     0       0   0   \n876559    0     1    0          0       0      0     0     0       0   0   \n876560    0     0    0          0       0      0     0     1       1   0   \n\n        organic  ounce  oz  pack  sugar  tea  itemid  \n0             2      0   1     0      0    0   37138  \n1             2      0   1     0      0    0   37138  \n2             2      0   1     0      0    0   37138  \n3             2      0   1     0      0    0   37138  \n4             2      0   1     0      0    0   37138  \n...         ...    ...  ..   ...    ...  ...     ...  \n876556        0      0   0     0      0    0   38934  \n876557        0      0   1     0      1    0   38250  \n876558        0      0   0     1      0    0   29571  \n876559        0      0   0     0      0    2   26244  \n876560        0      1   0     1      0    0   36432  \n\n[876561 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bag</th>\n      <th>bags</th>\n      <th>box</th>\n      <th>chocolate</th>\n      <th>coffee</th>\n      <th>count</th>\n      <th>dark</th>\n      <th>free</th>\n      <th>gluten</th>\n      <th>no</th>\n      <th>organic</th>\n      <th>ounce</th>\n      <th>oz</th>\n      <th>pack</th>\n      <th>sugar</th>\n      <th>tea</th>\n      <th>itemid</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37138</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37138</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37138</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37138</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37138</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>876556</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38934</td>\n    </tr>\n    <tr>\n      <th>876557</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>38250</td>\n    </tr>\n    <tr>\n      <th>876558</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>29571</td>\n    </tr>\n    <tr>\n      <th>876559</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>26244</td>\n    </tr>\n    <tr>\n      <th>876560</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36432</td>\n    </tr>\n  </tbody>\n</table>\n<p>876561 rows × 17 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.merge(title_features, on = 'itemid', how='left')\ndf_train = df_train.merge(summary_features, on = 'itemid', how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train['also_buy'] = train['also_buy'].replace('[','').replace(']','')\n# also_buy = train['itemid'].join(train['also_buy'].str.get_dummies(sep = ','))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.vote.fillna(0, inplace = True)\ndf_train['overall'] = df_train['overall'].apply(lambda x: int(x))\ndf_train['rating'] = df_train['overall'].apply(lambda x: int(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.to_csv('df_train.csv') #checkpoint","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2.3.2 Further data preparation for LightFM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train = pd.read_csv('../input/last-attempt/df_train.csv') #load from checkpoint\n# df_test = pd.read_csv('../input/last-attempt/df_test.csv')\n# with open('../input/last-attempt/model(1).pkl', 'rb') as f:\n#     model = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features_user_train = df_train[['userid', 'verified', 'vote']]\nfeatures_item_train = df_train[['itemid', 'main_cat', 'five',\n                                'good', 'great', 'love', 'star',\n                                'tast','bag', 'bags', 'box', 'chocolate',\n                                'coffee', 'count', 'dark', 'free', 'gluten', 'organic',\n                                'ounce', 'oz', 'pack', 'sugar', 'tea']]\ndf_train = df_train[['userid','itemid','rating']]\ndf_test = test[['userid','itemid']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Item featurs building"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_items(item, features):\n    item_f = []\n    col = []\n    unique_f1 = []\n    for column in features.drop([item], axis=1):\n        col += [column]*len(features[column].unique())\n        unique_f1 += list(features[column].unique())\n    for x,y in zip(col, unique_f1):\n        res = str(x)+ \":\" +str(y)\n        item_f.append(res)\n        print(res)\n    return item_f\n\nitem_f_train = get_items('itemid', features_item_train)\nuser_f_train = get_items('userid', features_user_train)\nitem_f_test = get_items('itemid', features_item_test)\nuser_f_test = get_items('userid', features_user_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightfm.data import Dataset\n# we call fit to supply userid, item id and user/item features\ndataset1 = Dataset()\ndataset1.fit(\n        df_train['userid'].unique(), # all the users\n        df_train['itemid'].unique(), # all the items\n        user_features = user_f_train,\n        item_features = item_f_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(interactions, weights) = dataset1.build_interactions([(x[0], x[1], x[2]) for x in df_train.values ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_pattern = [x + ':' for x in features_item_train.drop(['itemid'], axis=1)]\n\ndef make_feat_list(llist, pattern):\n    result = list()\n    for x,y in zip(item_pattern,llist):\n        res = str(x) +\"\"+ str(y)\n        result.append(res)\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ad_subset = features_item_train.drop(['itemid'], axis=1)\nad_list = [x.tolist() for x in ad_subset.values]\nitem_feature_list = []\nfor item in ad_list:\n    item_feature_list.append(make_feat_list(item, item_pattern))\nprint(f'Sample: {item_feature_list[0:5]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_tuple = list(zip(features_item_train.itemid, item_feature_list))\nprint(f'Sample:{item_tuple[0:5]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_features_train = dataset1.build_item_features(item_tuple, normalize= False)\nitem_features_train.todense()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Building user features"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_pattern = [x + ':' for x in features_user_train.drop(['userid'], axis=1)]\nad_subset = features_user_train.drop(['userid'], axis=1)\nad_list = [x.tolist() for x in ad_subset.values]\nuser_feature_list = []\nfor item in ad_list:\n    item_feature_list.append(make_feat_list(item, user_pattern))\nprint(f'Sample: {item_feature_list[0:5]}')\n\nuser_tuple = list(zip(features_user_train.userid, user_feature_list))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_features_train = dataset1.build_user_features(user_tuple, normalize= False)\nuser_features_train.todense()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3 Model creation for production"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# dictionaries of user/item/features names\nuser_id_map, user_feature_map, item_id_map, item_feature_map = dataset1.mapping()\n\nmodel = LightFM(\n    learning_rate=LR,\n    loss=LOSS_FUNCTION,\n    no_components=NUM_COMPONENTS,\n    learning_schedule = LEARNING_SCHEDULE,\n    random_state = RANDOM_STATE\n)\n\nfrom tqdm.notebook import tqdm\npbar = tqdm()\n\nmodel.fit(interactions, # spase matrix representing whether user and item interacted\n    user_features = user_features_train,\n    item_features = item_features_train, #user and item features sparse matrices\n    sample_weight = weights, # represents how much users and items are interacting or ratings\n    epochs=NUM_EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.3 Precision and recall at k score calculation"},{"metadata":{"trusted":true},"cell_type":"code","source":"prec_score = precision_at_k(\n                     model,\n                     df_train,\n                     num_threads=NUM_THREADS,\n                     k=10,\n                    item_features=item_features_train,\n                    user_features=user_features_train).mean()\n \nrecall_at_k = recall_at_k(model,\n                     df_train,\n                     num_threads=NUM_THREADS,\n                     k=10,\n                    user_features=user_features_train,\n                    item_features=item_features_train).mean()\n\nprint(recall_at_k,prec_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2.3.4 Pickling model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create an variable to pickle and open it in write mode\nmodel_pickle = open('model.pkl', 'wb')\npickle.dump(model, model_pickle)\nmodel_pickle.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.5 Predict/submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"user_ids = df_test.userid.apply(lambda x: user_id_map[x])\nitem_ids = df_test.itemid.apply(lambda x: item_id_map[x])\npreds = model.predict(user_ids, item_ids, user_features=user_features_train, item_features=item_features_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds.min(), preds.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_preds = (preds - preds.min())/(preds - preds.min()).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"normalized_preds.min(), normalized_preds.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/recommendationsv4/sample_submission.csv')\nsubmission['rating']= normalized_preds\nsubmission.to_csv('submission_log.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission_log.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.6 Extruding embeddings for application"},{"metadata":{"trusted":true},"cell_type":"code","source":"item_biases, item_embeddings = model.get_item_representations(features=item_features_train)\nuser_biases, user_embeddings = model.get_user_representations(features=user_features_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"item_emb_pickle = open('item_emb.pkl', 'wb')\nuser_emb_pickle = open('user_emb.pkl', 'wb')\npickle.dump(item_embeddings, item_emb_pickle)\npickle.dump(user_embeddings, user_emb_pickle)\nitem_emb_pickle.close()\nuser_emb_pickle.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install nmslib","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nmslib\n#search graph\nnms_idx_i = nmslib.init(method='hnsw', space='cosinesimil')\nnms_idx_u = nmslib.init(method='hnsw', space='cosinesimil')\n \n#adding items tograph\nnms_idx_i.addDataPointBatch(item_embeddings)\nnms_idx_i.createIndex(print_progress=True)\nnms_idx_u.addDataPointBatch(user_embeddings)\nnms_idx_u.createIndex(print_progress=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#getting nearest items on the graph\ndef nearest_items_nms(itemid, index, n=10):\n    nn = index.knnQuery(item_embeddings[itemid], k=n)\n    return nn\n\ndef nearest_items_nms_u(itemid, index, n=10):\n    nn = index.knnQuery(user_embeddings[itemid], k=n)\n    return nn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nbm = nearest_items_nms_u(14112,nms_idx_u)[0]\ndf_train[df_train.itemid.isin(nbm)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3. Conclusion"},{"metadata":{},"cell_type":"markdown","source":"* Score действительно падает при добавлении матрицы фичей, но по моему мнению все равно важно опираться на признаки относящиеся к составу и специфике продукта. В этом плане достаточно информативными оказались заголовки, токенизацией которых я и генерировал большинство признаков. Поэтому я не делал упор на достижение сильно большего чем бейзлайн показателя.\n* В качестве метрики я бы скорее использовал не roc auc, а precision at k и recall at k. Так как класс \"релевантных товаров\" для нас интереснее чем класс \"нерелеватных\". И нам интересно насколько хорошо модель вычисляет именно релевантные товары при данных базовых вероятностях.\n* Проблемой оказалась достаточно большая ресурсоемкость предподготовки модели и самой модели, что отчасти решалось удалением неиспользуемых переменных."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}